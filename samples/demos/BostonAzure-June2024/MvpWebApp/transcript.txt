Pamela Fox: Building a RAG app to chat with your data. This is a recording of the March 28, 2024 meeting.One of the most popular use cases for generative AI is RAG (Retrieval-Augmented-Generation), a technique that feeds context to an LLM (large language model) so that it can answer questions according to sources. Thousands of developers have deployed RAG applications to chat with their data, both for internal enterprise data (like HR or support) and external public data (like government or retail). In this talk, I'll show how to build a RAG with Azure technology, like Azure AI search and Azure OpenAI, and share best practices for data ingestion, orchestration, and evaluation.Pamela FoxPamela Fox is a Cloud Advocate in Python at Microsoft. Previously, she was a lecturer for UC Berkeley and the creator of the computer programming curriculum for Khan Academy. She has founded GDI chapters globally and develops curriculum for GDI.Slides: https://speakerdeck.com/pamelafox/building-a-rag-app-to-chat-with-your-data-on-azure. Json the data format thank you Jason for the introduction uh I do have the slides available for those who want to have a copy of the slides or follow along with them you should be able to get them from that speaker deck link so today we're going to talk about large language models briefly just to you know set the stage um for that terminology then go deep into rag which is retrieval augmented generation uh go and look at this open source rag chat app solution that can be deployed on as your talk about how to evaluate rag chat applications and talk about observability for rag chat applications on Azure so just to start off with talking about llms uh so llm stands for large language model it is a model that's so large that it achieves general purpose language understanding and generation so example I like to give is sentiment analysis which is you get a you know a sentence and you say whether it's positive or negative in the past what we would traditionally do is we' train a custom model with you know trained data that said here's a sentence it's positive here's a sentence it's negative and then what we would get is a model that could classify sentences as positive or negative and we would specifically use that Sate and now model when that's what we needed but with an llm we can just do this uh we don't have to specifically train the llm to know how to do it it just is able to do it because it's just seen so much data and been trained on such a large data set and one way you can think about the size of an llm is how how uh how many training operations it's had so the these graphs here are in turns of training flops which is floating operations per second I believe and what you see is that there's this jump when they've had 10 to the 24 training flops they suddenly achieve this ability to do really really well at these General purpose tasks and these are the classic ones that they do like modular arithmetic word in context Etc um so that this is observation they had is that if they threw enough data at it they threw enough compute at it then they you know achieve these general purpose models so you can there you can call them Universal models Foundation models large language models um but the point is that we can use them for a wide variety of tasks and that's really really exciting um and it's kind of like I don't know I feel like it's lowering the barrier for more folks to get involved with with uh Ai and using models because you know you think of a task you're like well maybe I can just use an llm to do it maybe I don't have to you know train one and learn how to do training well so there are lots of llms in used today ones that are both hosted on you know company infrastructure and also ones that are open source that you can even download to your computer and use uh so you know the hosted ones the big one of course uh are from open Ai and that's the GPT model like gp35 gbd4 whatever GPT is coming next uh Google is also in the game here you know it put out Palm was like a year ago and now it's put out Gemini and Gemini you know looks like it might be pretty impressive uh anthropic is a company I think they forked off of Open Ai and they recently put out the Claude 3 family which seems to be pretty powerful and then we've got the open model so there's a lot of open models coming from meta like uh the llama llama model uh and that one has kind of variety of sizes and then there's also this company mistro AI that's been putting out some models as well uh yeah I see a question from so yeah can you go back to the last slide and just want to understand when you say this um um like model scale training flops so what does it mean like mod AR arthamatic mod task nlu and word in context what does it mean these tasks that they get the models to do so mod arithmetic this is modular arithmetic so typically we wouldn't expect like a language model to be able to do modular arithmetic since it's not trained for math uh but you can see that its accuracy does jump to like 35% when it has been trained on a lat enough data just because it ends up seeing modular arithmetic a lot in the data um this one is multitask natural language understanding I don't remember exactly what that test test looks like um and then I don't remember what word into context looks like so I'd have to dig up the paper uh to remember what these what these tasks look like it's a good question thank you thank you so much uh yeah so and I see a good question about hallucinations so I'll certainly talk about that throughout it um so what we see is there's a variety of LMS so some of them are hosted and these are the like the most powerful ones you can really only use on someone else's infrastructure um unless you're willing to go through a lot of effort to figure out how to you know host one of these powerful and set up the GPU and all that stuff but you can totally get started running with these smaller models locally I recommend using olama you can see I've got like a little llama up here uh and here I've got it running I typically have it running over here yeah so let's see emojis for Boston I always use it to generate emojis uh so this is with llama 2 so I just write a llama run llama 2 and what that does is that it pulls down the model if I don't have it already uh and then it lets me just chat with the model so here are some emojis to represent Boston so we get a building a park and a person walking they're pretty good ones I like to walk around Boston when I go there uh so I do recommend playing around with olama it's the easiest way to get started with these open models locally and it's just fun to have an llm in your terminal they're not as powerful as the models you'll definitely get more you know hallucinations um as you know Lucas was saying um but uh there's a lot you can do with it h you know especially if you just need a quick a quick Emoji quick poem that sort of thing so one thing that you might want to know about terminology wise is GPT what does that stand for it is generative pre-train Transformer that's what the open AI models are named after you'll also just generally people talk about he people talk about Transformer architecture so that architecture was actually first described in a research paper from the Google brain team so it's funny that you know open AI came up with the gvd models first but uh you know things happen uh so if you're interested in seeing how this architecture actually works and you like reading research papers uh do check out that paper that's where I got this diagram from um but basically this Transformer architecture m let uh was much better for language because of its ability to see multiple parts of the like kind of input the sentence at the same time so that's my very simp simplified and probably not quite correct uh you know understanding of the Transformer architecture there's more in the paper and there's also some great videos for f from folks that are actually machine learning experts that explain the attention architecture more there's also a fantastic videos from Andre karpathy who is a machine learning expert and he was just uh he was previously at open aai and at Tesla Ai and he's got a great talk state of GPT and then also he has a series where he builds GPT from scratch in code using python notebooks so that is a really cool Theory I still haven't made it through the whole series because it's a little intimidating to build entire tvt it takes quite a few hours but every time I watch his videos I I just learn so much so if you want to dig into how these you know how these GPT this transform architecture actually works that is what I recommend so how do we actually use these uh these GPT models from open AI uh one way you can do it is an Azure Studio I'm guessing some of you have already done that um so please you in the chat uh let me know what your experience so far has been in terms of using open AI as your open AI that sort of thing uh so there is this you know Studio that you can use you can get to it from the um from the portal now the first thing you need if you're going to use as your open AI is that you do have to fill out a form in order to get access and in that form you have to say what your usage your expected usage is going to be so that Microsoft can say like okay that sounds like a good responsible use of um of open Ai and then once you got it then you can deploy all these models you can see I've got quite a few I have quite a few deployments because I spend way too much time doing this stuff um so let's see I can go to my model deployments and then open those up in as your open AI Studio load load load oh here we go oh I'm just gonna all right so I'll go to as your opening I Studio there is also something called a your AI Studio which is different from a your open AI Studio I tend to only use a your open AI studio um but a your AI studio is where you'd go if you're you know using other models making your own machine learn models that sort of thing I think eventually they're going to merge the two but right now we've got two different Studios so this one's really fun just the chat playground it's just a you know just a wrapper for playing with the API so we can be like you know right haiku about Boston and it'll send it off and we can see Harbor City's charm Boston's history unfolds proud Spirit endures okay so that's cool but maybe we want to change the system message so how people find more you know you're AI assistant that uses so many emojis okay so I'll change the system message I'll apply the changes and now I'm going to ask ask it to write a haiku about Boston again and there we go we can see a lot more emojis let's also we're talking about hallucinations we can say what is the weather in Boston and those of you who are in in Boston well there you go um it's still doing hus and it's G Gabby a range here so what is the temperature today I'm trying to get it to lie basically uh okay so sometimes it does this response where it says I'm sorry it's an AI I don't have real time data access so that's good uh it's you know it's better when the LM doesn't try to make something up but let's just try to force it so I'm going to change temperature to one and top P to one to try and increase its creativity level and we'll see yeah it still is refusing to answer that's good so sometimes you can get the llms to to uh make stuff up and uh depending on you know how it's been trained sometimes it won't do that for you all right so that's a little playground and you can also grab code from here so once you've played around with something you can actually grab the code uh you can get it in you know they've got different ones they've got curl C Python and Json I'm not sure what the Json is for I typically use Python and so most of the code you're going to see me using today is in Python so as you see here here is an example in Python uh so we pass in the conversation so when we're working with these chat tuned models so we call them like chat tuned models uh so like CH gp35 and gbd4 are chat tuned which means they expect their input to be to look like a conversation so we typically start off with a message which is the system prompt so it has roll system and then it has the system prompt and that gives it just overall tone format expectations uh and then we have the question from the user and then and we can also keep going so we could pass in you know multiple back and forth um so that the you know the chat model could see what messages happened before uh we can choose to stream the responses or just get the whole response back at once so llms can be really great you just saw we could get it to know write Haus and use poems and all that stuff uh but you know as Lucas was saying like you know there's uh it can make stuff up right and it can answer things incorrectly and why is that well one is that llms are have always have outdated public knowledge right because they train the llm you know on the internet up to a certain point and then nothing after that right so if we asked about something recent that happened it would you know it would have no idea uh the other thing about llms is that they do not have access to anything internal so if you know if you have data that's inside your internet there's no way that an llm is going to know anything about that so if you're trying to ask company specific data it's just not going to know right so those are you know the limitations we run into and then we try and figure out like how can we work around these limitations so there's you know three General techniques to incorporating domain knowledge the first technique is to do try to do prompt engineering this prompt engineering is only going to work if the data is inside their inside the weights somewhere right like let me see if I can do an example of this um so I'll say um write SQL Alchemy code uh that's a particular python package for SQL databases okay so yeah so it did this this is the old way of writing seal Alchemy code uh I'm going to see if I can get it I don't know actually if it's seen the 2.0 I think it has code using the declarative base C Class let's see if I can get it to fix itself oh it says squal 2.0 doesn't exist as of my knowledge so yeah unfortunately in this case it can't do it so what I was trying to show is the the fact that if if you know imagine there's you know two different versions of a library in an llm weight you can often get it to you know pick the most recent one if you're giving it a hint that that's what you're trying to get it to do right but that only works if it's in the weight somewhere right in this example it actually didn't even have anything in its you know in its training data at all so we couldn't get it to come out with the code so prompt engineering is generally not going to be that helpful when we're trying to incorporate specific domain knowledge only helpful if it is actually somewhere in those weights and we're trying to steer it the next approach people talk about is fine-tuning that's where you actually are kind of training a subset of the weights and so you would repair like you know maybe thousand examples or or more and those examples would be you know examples of uh you know user questions and chat answer and in that way the llm could actually learn new skills permanently so it's a very it's a it's a you know effective way of getting an llm to you know update it's weight to learn something more however it is expensive right it's expensive to do the fine tuning and then it's more expensive to run uh to you know to use a fine tune model than to use a base model so I actually personally have not done any fine tuning myself I haven't felt the need but uh and and a lot of people have talked about where you know uh you could do fine tuning or you can just use the the the you know the most recent model and and that is generally going to be better so fine-tuning we like to consider a last resort because of the ex you know expense involved you know the time and all that stuff um so it is it is a tool in our toolkit but it's not the thing we should necessarily reach for immediately the technique that I like to use is retrieval augmented generation which is a way of just giving the llm the facts just in time right um you know for example with the SQL Alchemy one that I just showed if I you know wanted to make sure it really did um you know answer something correctly well I could just give it information I could just say like well you know um you know here write a model for a restaurant class based off this example right so I'm basically going to like give it current code and and see if it can learn how to do it yeah it got a little bit better there yeah uh so the idea here is that you know we're we're going to give the llm the information to answer a question based off the domain data and we're just get get going to give it to it at the point where it's answering the question so that it can see what we want to answer the question based off and it can still use its skills of answering questions on that provided data so I have one question if you don't mind previous so where does the few shot fit into in this um yeah that's a good question um yeah I should I should do like an updated version of this I actually I did talk about that in a presentation on um Friday uh so me find yeah ways to improve llm output so F shot examples uh so we could even do that in the playground too you can add examples and this shows the chat what responses you want uh so I would say F shot examples are the most useful for format they could also help like in this case with SEO Alchemy like it might be enough to get it to write the syntactically correct SEO Alchemy code but usually few shot examples is more about learning the format that you're looking for and less about the knowledge um because typically your F shot examples are going to be the same across all the questions but the knowledge for each question would actually be different um does that make sense yes so that means a fuse shot would be for the formatting and for the you would use the rag or fine tuning if it is knowledge based is it yeah yeah and I I can show examples because we do use few shots for our you know for for formatting to try and show the LM this is the kind of format we expect so we use fuse shots in combination with rag so it's definitely a good tool to use f shot examples especially when you're trying to get a particular format particular length that sort of thing um but it's uh I would say it's complimentary to rag got and and irres of which llm we use is that a same how we interact with the with the foundation models in terms of let's say the F shot is the way that we do it for GPT would it be the same for the clot um do they follow the same Pro you know similar way to interact with them yeah generally generally that would be similar um you know you know you see people talking online online about what prompt engineering and few shots they're doing for Claude and and other models there are some differences in terms of um if you're going to do something with like function calling or Json output so there there's a a lot of people try to get the llms to Output structured data uh like if you're trying to get it to Output a Json that um you know like you pass it in news article and then you say output a Json that has the title of the article the top you know cities mentioned in the article and the top people mention the article right so doing like an any analysis to get Json out if you're trying to get Json out of an llm that actually is is going to vary based off the model because some llms have like specific uh kind of API endpoints in order to to get Json output because they've been they've tuned it for that uh and with like the open AI use the tools parameter to the API um H but not all the not all the API support that so the main difference I've seen so far is if you're trying to get structured output out of the llm the way you do that per model uh may vary okay thank youh all right so retrieval augmented generation let's dig into this so let's say we have this example here do my company perks cover underwater activities and I actually have I have this one open let's see is it over here there we go yeah I'll even try it again do my company perks cover underwater activities let me zoom zoom zoom so it is going off and trying to figure out the answer to this question and it should be doing it in and this one's this one is actually running locally okay so there we go we see underwater activity such as scuba diving are covered under the perks plus program now this is a chat that's specific for a fictional fictional company's data and it's managed to answer this question and it does that using retrieval augmented generation so how did this actually work so we get the user question and we use that to search a knowledge base right so this knowledge base it's either a search engine in this case it's as your AI search it could be a database we're searching some sort of SE search engine it could even be a numpy array actually like if you had a small enough amount of data it could even be like an inmemory array but you need to search your data somehow based off this user question in order to figure out what are the pieces of knowledge that could help an LM answer this question and then we take that you know we get that information back and we take that and the original user question we send it to llm and say hey answer this user's question based off this data and make sure you site your sources and then we get back this answer that says yes your perks do cover some under activities like scuba diving lessons so this is the general approach to rag is that we're getting the search documents back the search results back and we're passing both the user question and the documents to that llm call and we can actually see that here if we click on the thought process so first we uh you know we search using this query and we get back search results from as your AI search and these are Snippets from employee you know employee HR handbooks type stuff and we can see you know the page number and all that stuff so we get back these and then this is the actual conversation we send to the model so we have the system message and the system message says You must do it according to the data and the sources below then we have the user question and actually here is where you can see we do have few shots so we have an example of a question and sources and an example answer and that's to show it how to do citations right because we're trying to get citations in a particular format just trying to make it bigger bigger bigger right so we're saying like Okay this is an example this is how you answer this is you know how you give those citations and then and then we have the actual user question and the sources that we got back and these are just concatenated you know with new lines and that's it that is what we send to the llm so that is the heart of retrieval augmented generation is that you send the the information along with the question and the LM is able to look through that information in order to synthesize an answer have a question or okay can can I ask a question yeah so P how like what is this is this a python based um web UI you have developed and then how you have developed like what what you are actually trying to achieve from this how you have developed this UI if you can explain that it will be really great yeah so I'll dive deeper into the actual code for this this is a this is all open source it's a react front end with a python back end and I will I'll I'll step through the code as part of this presentation in a bit sure thank youh all right so as we can see yeah yeah have one small question so when it comes to rag um and this might be just you know your experience but how do you balance the amount of data you're actually putting in either that Vector DB or you know I'm kind of trying to understand this because I've heard these scenarios where people throw in just mountains of PDF files in a vector DB does that then I get to the point like well can't I just search the vector DB and what do I need the llm for so I'm trying to understand the kind of like the balance between you know how much data you have in there or the quality of the data or does it even matter I mean you know for example in your case here you have the HR information do you just basically put the entire every single PDF file you possibly can find and you just scrape it and put it in the vector DB and you're pretty much going to be getting quality results or do you you know how do you balance all the splits or whatever comes in I'm just trying to get a better understanding of that if that kind of makes any sense yeah so here you can see these are the only bits of like chunks of documents uh so when we're doing rag on documents we do split up the documents uh into chunks that are about this this size here about like 500 Words uh so we we split up the documents into these chunks uh because the whole thing is that we don't want send too much I actually think that might be my next slide so okay okay okay I can hold us uh so yeah so as we see rag is beneficial okay so yeah let's talk about that search step and the what the data looks like right um so we need to get good results to the llm or we're not going to get a good answer and yeah another point is that what is the llm actually doing right the llm in this case it's synthesizing information so we're basically I think of brag is making search more accessible right so people they don't want to have to search and click through multiple documents and like synthesize information in the head so we are using an llm to synthesize information and present it to the user in a concise way um but really it's it's it's really like kind of just you know making a more friendly search interface and it turns out like people like people like having a more friendly search interface where they just feel like the answer is right there in front of them um but in fact like we do need to focus a lot on that search step because in order to get a good answer synthesize we need to have really good search results right if we if we give the llm garbage it's going to Output garbage um if we give it gold you know it it it it should you know output gold uh so we want to figure out how can we really make those you know those search results be really helpful for that LM so our general goal is to come up with a small number of searches uh search results that contain the answer uh we don't want to like we don't want to find too much information because if we send too much information to an llm it does tend to get a bit lost like either if we send too much information either it just won't find the answer at all because it'll kind of just ignore it if it if it's just too much being sent at it uh or if that if we send so much information that we kind of have conflicting answers in there and we have some information that's like misleading in there and doesn't actually answer the question the LM could get misled and answer the the thing wrongly right so we really want the search step to find the most relevant documents and nothing more uh and not too many documents right so in my example I'm only retrieving three documents between three to 10 would be the usual um so that's we we don't want to send too much this there's this paper here lost in the middle uh where they were you know looking at how many documents you could send and basically like as you sent more and more the accuracy went down because the llms would you know find it too noisy to find the answers in that large chunk of information uh so you can see here we only got the three document chunks back and uh and and you know these chunks contain the answer and they don't contain too much more that is that's the goal um as like you know how many documents do you throw into your search index I mean you do want to throw in everything that could possibly answer a question that a user would have I I I think that's that's true you just want to throw it in in a way that you're going to be able to to you know search well um on that data later uh because you don't want to just send your you know too much information at an LM so when we're doing the actual search uh we want to use the best search strategy for searching with the you know use your question uh these days everybody's talking about Vector search a vector search it means that you turn the user query into a vector and uh you know Vector using like the open AI uh embedding models and it basically like that Vector encodes all this kind of semantic stuff about what's what's in that text so if you had a user question about dogs then that would end up you know in this similarity space where it's similar to cats because dogs and cats are both pets so they are sem atically similar even though they're spelled completely differently right uh so you know this example here I asked about underwater activities but the word underwater is nowhere in the text at all instead the word scuba diving is in the text but underwater and scuba diving are both similar in the vector space so that's why everyone's talking about Vector search because it's a way that we can uh you know we can get results for things that are semantically similar even if they're spelled completely differently so we definitely want to do a vector search so that we can you know find those semantically similar things we still want to do a keyword search because Vector search is not good for some things and this is something I want to stress because there's been such an obsession with Vector search because we're like we're all like discovering it now that people are forgetting about keyword search but oop um but keyword search is still necessary for some stuff like if I search for like an exact you know number like you know what costs you know $1 19991 1999 is not going to do well in a vector search or if I search for an exact name like you know everything about hamla Susan Fox I want that to be done with a keyword search that's an exact name I don't want you know things that people that have names like me right I want Pamela Susan Fox so we still want to have keyword searches done as well so we need a search strategy that's going to do a vector search going to do a keyword search and then is going to combine those together right because you get both those results right so you search using your you your full text query you search using your vector query you get back results you need to somehow merge those results together you know remove duplicates figure out um you know which which scored higher and then you want to you know you need to have some sort of way of ranking the vector versus the keyword search so what uh people often do is they use a reranking model so this would be a machine learning model where you say hey here's the search results here's the user question could you please rerank these search results in the optimal way now this seems like a lot of work and it it is actually because I was actually trying to implement this this morning myself so typically we I use as your AI search because it just does all of this for you and it's very very good at it uh so I do recommend as your AI search but if you were going to do this on your own database then you know you have to implement it on on that database and you can often find examples of how to do that um but the thing I want to stress is that you want to get the best results and a small number of results and generally the way to do that is with a really good hybrid search so you whatever you are using as your search engine make sure that you can get good results from that using a hybrid search uh I see a question from Robert in the chat is there way to leverage a company ontology for vectors and tonies for keywords uh like we provide documents to the llms um yeah that's a good question when you just do the vectors you're generally going to use like the open AI models because they're quite they're quite good if you had your own ontology I mean one thing that's interesting that you could link into this isn't publicly out yet um but I think it should be out in the future this is called graph rag because it sounds like you know you've got an ontology ready uh so let me link to this here um and it actually like in order to use graph rag I think you have to actually Define an an antology and uh they they say it works it can work better for some some types of rag uh so it'll be interesting to see I haven't had a chance to play with it myself um yeah that's a good question um you could also do like custom waiting with res searches and stuff like that but I haven't dug much more into it beyond that uh rajes yeah I have a question suppose you have a millions of Records in a database then you are going to go through millions of record create a vector out of it and then merge the keyword I think then that case going to be very very slow uh for the model to search those many records isn't it so yes yeah yeah you're right so doing a Brute Force Vector search would be quite slow um what uh you're usually going to use an index either hnsw or IVF uh those are the two main ones uh like as your AI search uses hssw so you just tell it which kind of index to use and this does as you can see fast approximate nearest neighbor surch so it's using heris in order to be able to quickly search um across so this is the current one that's the most popular and that's supported really well there's also IVF and there's also I was even reading uh people are really experimenting with this space and how to how to do uh Vector search faster so this is an article from Nvidia about doing uh GPU powered Vector search so it is a very hot topic uh but you can generally get pretty good performance with hnsw and uh you can do it with Azure AI search you can do it with postgress um so you can use one of those indexing algorithms with whatever database you use I I was watching some videos nowadays people are talking about graph databases uh yeah I saw somebody post on LinkedIn about doing what is it like neo4j or graph databases with rag I didn't read into it to see what particular approaches they they were using uh but uh that is definitely something look into so I don't know Mindy if you have any videos or articles you want to put in the chat that would be very helpful all right uh and I do also recommend reading this blog post from the Azure AI search team uh so aka.ms rag relevance also just put it in chat this nice chat here and it really nicely demonstrates uh you know the differences between just doing Vector just doing keyword and doing you know hybrid and ranker and and you know for those of you interested in like you know how do you decide how to index documents this is just a really uh really fascinating article so what does it really look like to do a rag with hybrid search so this is just my diagram before but broken down a little bit more right so we get the user query we take that user query and first we actually send it to an embedding model like Ada 002 or even one of the new embedding models from open Ai and we get back that vector and so then we take the vector and the text and use both of them to do a hybrid search which will get back the merge results and then you know with those result results we send both the results uh and the user query to the large language model and get back the the answer so this is still actually even simpler than what we actually do in the app but I'll I'll talk about that soon um but as you can see there's various steps involved here in doing a proper rag with a hybrid search now another thing to think about is what is your rag searching uh is your rag searching documents or you know I would think of as unstructured data or is it searching like rows of an existing database right so imagine uh you know you have like a an online store and you wanted to have a rag where customers could ask questions about your store items right so those I think of as actually being two fairly different rag situations and a lot of times when people are talking about rag they're talking about rag on documents because that's I think where people are really like I don't know I think maybe because it's harder and that's where we tend to focus on it because it's you know it's it's you know trying to figure out how to ingest all those documents but you could also totally do rag on database rows and um and you can have a really com compelling experience with that too so do you have any articles I actually that's one of my um kind of a research item to do uh do you have any articles you could share on how to do database Rod uh yeah that's a good question um I was actually working on it this morning uh so I intend to put out various samples so you can see um post from this morning so the first thing I did was I implemented a hybrid search for postgress um you know postgress with PJ vector and so you can see the search right so I pass in a text and a vector and then I get back to results so if you can Implement hybrid search on your database um you know it means you need to have some sort of vector support with so with postgress I use PG Vector um various database extensions of different ways of storing vectors so hopefully there's one in your database of choice once you do that it was actually really easy just to swap it in to my um you know to the app that that I've been showing I just kind of substituted the Azure AI search call with the call to the uh hybrid search endpoint for the postgress database uh but I will be putting out uh some repos around this to make it easier for folks so I guess want to be clear so it's it's like you're able to scan the whole database schema and then ask questions in natural language or did you just okay so when I do I I said best shoes and that um that turns into a a hybrid search query on the well in this case just on the shoe table so I did have to decide what is this going to you know what Fields that's the thing you have to decide if you're going to be doing rag on database rows you have to figure out which what are your target columns like are you going to be actually searching across all the columns or just one column and when you make a vector are you going to make a vector for each of The Columns and like search on each of those vectors or what a lot of times people would do is you know they kind of you can do like stuff it so you you would concatenate all the relevant columns together uh and then compute the vector for that so you would still just for each row have a single a single column for the embedding for that row as long as it encoded all the stuff you thought would be semantically relevant so that's that's actually the decision that you have to make is uh you know how many how many embedding columns are you going to have and and what is your you know full Tech search going to search as well yeah this a great question and definitely something we need to talk more about and have more examples for uh so so yeah if you're doing database rows you do need to figure out a way to have Vector support um for the ability to have a vector column and to have efficient search right we were talking about indexing right so you need a database that has support for Vector indexes so you're looking for support for hnsw or IVF those are like the efficient Vector searches uh or just looking for a native Vector database um you know that's an option too so on aure uh the options we have here are you know uh for database rows you could do as your AI search if you wanted to copy the data over that's also an option if you're okay with having a copy of the data as your AI search can connect to various databases and and index it uh but then you'd have a copy of the data so you know if your data changes a lot you might not want to do that uh you can do as your post grass flexible server plus PG Vector you could do as your cosos to be plus their new Vector support you could use container app Services uh with these built-in Vector databases like milis quadrant or weate uh and of course you could use the open AI embedding models in order to actually you know compute that column so that's rag on database Rose and I haven't done that as much as saying like I only really did it this morning to make sure that you know to to kind of prove it um what I've spent much more time on is rag on documents which we can also think of as unstructured data so that's just whatever documents you have PDFs docs PowerPoint HTML markdown IM images all of those are things you could potentially uh you know ingest into a search engine but for that you do need an ingestion process that's going to take that document extract the data from it split it into good siiz chunks because remember we don't want to send too much information at an LM then you know compute vectors for each of those chunks and then store those so on Azure the best option for that is definitely a your AI search uh in combination with document intelligence for the raction and open AI embedding models for the vectorization uh so how would you actually build a rag chat app on Azure there are various Solutions ranging from no code to low code to high code my emphasis is always high code but I will show the no code and the low code options because hey if they get the job done that's great the no code option is co-pilot studio and you know is is really meant to be you know a user friendly way of setting up your own co-pilot so you can say like oh here's you know make a co-pilot based off of this blog right so I tried making one based off my blog and it'll you know it'll try to do all the ingestion and indexing for you and give you this chat interface now the thing about co- pilot studio is that I actually don't really know how it works behind the scenes right it is uh it's you know kind of a black box uh I do work at Microsoft but I don't I don't actually know how to like find the code Behind these things so I actually don't know how is it doing the ingestion how is it doing the searching I don't even know actually if it's using gp35 or gbd4 um I'm trying to ask around just to find out for my own edification but generally if you're going to use a you know a no code option like this it's you know you're not going to be able to have that much flexibility right you're not going to be able to really um you know extend it if you realize it's not working for your use cases uh so it's always good to start off with these things just to see does it work for your use case especially if you have like a really low stakes use case um but then you know keep in mind that you'll probably reach a wall now the next option is actually a really compelling option because it actually has some code that you can mess with and that's that as your AI open AI Studio on your data so I think I still have this open here yeah so you can actually go to this add your data Tab and add a data source and you can click you know as your AI search if you have an existing index or you can use blob storage Cosmos CB elastic search I think they're even adding pine cone you can specify a web address upload files so they're trying to figure out what are the common data sources that people want to use and make it easy to connect to them uh so it supports you know all sorts of documents because you can use aure I search uh you can upload documents to it so lots of documents uh it kind of supports databases in that you could connect your as your AI search index to a database um but I don't think it's really uh kind of I don't think it's really Geared for databases I think it's a little more Geared for for documents but uh but there is a cosmo actually I mean there is Cosmos D for  VOR so yeah so you could try that out for for that if you've got your data there and uh and yeah and how does it do the searching it's got lots of different search options right so even some non- aier ones like elastic search and you can choose whether you're going to use three you know 3335 or four so this is a pretty cool one because actually once you you can make it and you can deploy it entirely from the studio and if you want to extend it you can actually get the code for it from their repo so I believe that this you know this is the code that powers it so if you do deploy it and you like it you can try to actually change the code if you need to make some tweaks now there is still limitations to this because I'll I'll show the code um what is doing let me find the relevant bit of code Source uh trying to find the chat completion call so a lot of options here chat. completions do create okay send chat request okay I haven't looked through their code in a while um stram okay let's see yeah okay it's kind of hard to find it but um what it does is actually using a particular parameter to the chat completion yeah these are basically the parameters so it's actually telling the chat completion API like hey here's the elastic search index use this index and search it so and behind the scenes that as your chat completion API knows how to do this this is only on aure you can't do this with open AI um you know on open.com you can only do this on Azure right now at least uh so it's doing the search for you so if you if this works for you great if you end up needing more flexibility over like the searching step and like how it's actually combining the search result with the you know with the system prompt and the user message and and all that stuff then this you know this may not work for you so you may reach a wall if if you find you need to uh you need to extend the actual rag flow more because this basically is like rag orchestration as a service so it there's code going on behind the scenes that's figuring out how to get the search results and how to you know how to answer a question based off the search results oh I see a question from Robert like what do you map it to from so you can map stuff like if you're doing like elastic search with you know with a your studio on your data you have to tell it what's the title column what's the URL column what's the content column what's the file name column so if you're you know if your data matches you know you know works with the assumptions they're making here like the assumptions here you can see is that you have a title and a URL and a file name and a Content so it's definitely assuming some sort of document-based rag but if if you're you know if that works for your rag use case then the as your opening eye on your data could work really well for you so lots of folks do start off in this ad data um and and deploy that and um and some of them you know stick with that and that works well for them uh and then if it doesn't then you can go to the full very high code uh solution and this is the one that I spend all my time on so I you know I know you know that on your data a little bit but I basically spend like way too much of my time on this on this repo here uh it came out in March of last year and it just became really super popular so you see it's got 5,000 Stars so it's actually the most popular aure sample in our aure samples repo so it's been deployed thousands of times so we've got lots of feedback from it we we're continually you know improving it you can check our releases to see you know the sort of things were changing um and it's all based off of feedback from you know a your developers as your customers so it's been a really great way to learn about how to make a rag chat application and see all the different ways people are trying to use rag chat apps and it is I would say the most flexible of all the options because uh you know it's it's open source and if something isn't working for you generally you can go in there and mess with it uh it is very much a high code situation like the the code is not necessarily simple we try to do that but it's not always going to work uh to make something simple but um you know it's very flexible and and that's can be the advantage of it so it currently it's geared for documents and for the search engine it's only as your AI search so that is certainly you know a limitation if you were trying to use like elastic search right um but generally aure AI search is the best way to search documents uh if you're you know using aure services and that's why uh we use aure AI search for the LM you can switch whatever you want uh so you can you know have these multi-turn chats you know whole conversations we've got user authentication buil in we've got Access Control built in so if you wanted some users to have access to different files than other users that you could do with this uh you can even use gbd4 with vision we have that as an experimental feature that you can turn on if you're interested in trying that out so a lot of the stuff are features that you can kind of turn on if you want to see if they you know if they work for you okay so now I'm going to dive into that solution so in order to deploy this you need an as your account and subscription uh you can use free account but it has an awful lot of limitations so usually better if not using a free account uh you do need access to as your openai or an open.com account you could use either of them you could even use olama if you really wanted I sometimes I do that just to see how the local models do but they don't do very well so I would recommend gbd3 5 or gbd4 uh you do need as your account permissions to be able to create the rback roles uh and to make the deployment so those are you know prerequisites to deploying uh and then to open the project you can use GitHub code spaces you can use VSS code with Dev containers we like to set up everything with a Dev container so that it's all all the requirements are set up for you or you can go ahead and set up a local environment if you enjoy doing that I do not and then you can deploy using the Azure developer CLI I don't know if you all have talked about the addd CLI before but it is by far my favorite way to deploy things to Azure uh basically what we do is we Define the infrastructure using bicep which is infrastructure as code files like terraform so we Define all the infrastructure in bicep that's just in the repo and that you know declares everything that needs to be provisioned and how they relate to each other and environment variables and all that stuff and then we just have to run you know ACD up in order to get everything provisioned and everything deployed uh in you know onto the platform of choice once it gets deployed what you end up with is is a bunch of azure services so for like the chat application itself that's on aure app service and it uses a your blob storage in order to render the documents and the citations uh it uses AZ your open AI for the embeddings and the large language model calls and it uses a your AI search for the searching we also do data ingestion in this repo and that one also uses all those Services as well as a your document intelligence so there's two ways that we do data ingestion so this is another opportunity we're going to talk more about data ingestion so if there's any additional questions uh I think from like George um this is a good opportunity to to ask them one question for data ingestion so if it's a document and it's unstructured I mean it's just text in a document so if you want to feed it text from your database I guess why would it need to have specific like vector indexing or anything special like that if you want to just you know all the fields of this one table you know are all the same type of text Data no different than you'd scrape it from like a document um so so you're wondering about scraping from a database yeah as data source yeah if you're using a database as a data source you do not need any of this right so I I probably still have my let me find my um here we go okay so if we were going to do you know um this this is like the code for my postgress hybrid search it's not perfect yet but but here I'm just doing uh you know so I just have like you know you you just need to have a database that has um you know you've got your columns here and then you've got an embedding and you have to decide whether that embedding is going to be the embedding of just like a single You Know Field like just the description or you could concatenate all these together and make it embedding based off that so that's probably what I would do is concatenate uh you know all like the text Fields together the things that kind of have semantic things in them concatenate in one thing generate the embedding based off that store that as an embedding column so you can see here like this is my schema um right so I've got all like my standard columns and then I've got a vector column for the embedding and then when I do the search I need to do the vector search with that column and then the keyword search uh right now I'm only searching one column but probably I should do that across multiple columns right and then I need to you know merge those results uh but yeah if you're working off a database you don't have to have all this fancy data ingestion you just really need to have uh you know ideally have an embedding column you could even just use I mean you can also just do it you could do just a full Tech search I I think you're you would be happier if you also had a vector search sech and a hybrid search on top um but you just need some way of searching your database based off a user query yeah that makes more sense okay thank you yep yeah great question all right so yeah so data ingestion this is really relevant to documents right imagine multi-page documents your HR documents that sort of thing right so we get the documents first thing we do is uh you know upload to blob storage actually that's even the last thing we do um uh so we get them to blob at some point just so we can and that's just so that when we have the actual app where we can um click on let me see so I'm going to click on I'm G to click on a bunch of citations and see which of them loads first because sometimes these PDFs can be a little low to SL to load okay there you see so it loaded the um this you know this PDF PDF here uh this one was this one is a rag based off my personal blog which is fun for me and so here you can see it actually loaded the HTML right uh so we you know we need these things stored somewhere so that we can uh you know show the citation so this is coming from blob storage and it's actually checking our um user authentication when it grabs it oh that one's going to be slow so we put them in Blob storage just so we can render citations um the next thing uh is that you know we have the documents and we need to extract data from them so uh we typically use aard document because it supports a ton of formats these days including docx and PowerPoint which was really huge for people when they added that earlier uh you know I think in the fall so it supports lots of documents even supports images it does OCR so it really tries to get as much text as it can get out of a document we do also have local parsers that we make available just because they can be cheaper or more customizable like for my blog I actually just use the local parser for HTML because I then I can just do some custom parsing for HTML uh and uh leave off the parts of my blog I don't want like the comments uh but generally the you know approach is to use as your document intelligence because it's very good at document extraction now we have the text extracted for the whole document but we still need it in chunks because we can't send like a 10-page document to an llm or at least we shouldn't because it's going to get lost right it's going to be too noisy for it so then we split the data into chunks uh so we have all this you know python code that looks for sentence boundaries and also tries to you know split things appropriately so that they're a good um you know a good size to send to an llm uh there's lots of other splitting libraries out there like in Lang chain that you could use as well then once we have the chunks for each of the chunks we compute an embedding of that chunk and then finally we put them in as your AI search uh and I can show you once again the results right so this is actually just straight from a your AI search uh so this is what the index looks like so we can see the content and we can see the embedding we've got like an ID uh we've got scores that come back from the search that's only relevant you know that we don't store that that depends on the query uh we've got what page it came from and we also have access control related information if we're using access control like uh the user ID and user groups so this is a local script uh so this is actually like just in the repo um if you know if I wanted to ingest something right so I'll go ahead and run it just to show you also Jason you can remind me if I'm like going way too long there's lots of good questions I no you're fine okay let's see all right I'm just going to see if I can ingest something just to show you okay I'll go here and let's see what environment I'm in right now okay it's a good one okay let's see if my ingestion script is working at the moment I'm always doing so much hacking on this repo that you never quite know what state it's in but on my machine it's good on Main okay so just running the the script here and it's just setting up well first it has to set up the environment to bring in all the you know the AZ your document intelligence SDK and all the other things we use for for the extraction ingestion any questions as it's running so I have quick question in terms of the cost wise right you know like you were saying earlier when when we use the fine-tuning it's definitely more expensive because there are tokens involved and lot in the same thing would it be more expensive when we use Rack or is there a way we can reduce the cost using any kind of caching in it yeah that's good question um I don't think you could use caching of user questions because I think your user questions would be so um so different i' I'd be impressed users were were actually asking the same thing uh generally like ways to reduce cost well I'll say this what are the expensive things uh you know there's app service but app service actually isn't too expensive it can it can handle a fair bit um without getting too expensive as your AI search does cost uh a bit especially if you're going to have if you need multiple replicas for the amount of traffic you think you're going to get and especially if you use the semantic ranker which is the ranker which is actually you know the best thing to do is to use the ranker but both of those are the things that make it the most expenses if you increase the replicas and you do uh keep the semantic ranker enabled so if you want to decrease aure AI search cost you disable the semantic ranker and you keep it one replica if that works for your traffic levels uh the your other approach with Azure I search is you know to replace it with a you know with a you know a database search engine um you know like the post one I was showing but then you do have to reimplement everything that as your AI search comes built in and they are putting a lot of effort into rag because rag is such a use big use case for them right now uh but you know that's just a decision to make and the other big thing that costs well as your document intelligence does cost money and and uh it can it's a that can be a decent amount if you have a ton of documents so if you're trying to reduce cost there you can use a local parser and see if the quality is good enough it depends on your kind of document um I don't think it's that the local ones that great for PDF but if you're doing HTML you could do a pretty good job parsing that H and then finally as your open AI cost money uh you that one you can't really reduce the cost except for just making sure you're not sending too much data right so it it's per token so the less you send the better right so if I do three search results that's generally going to be cheaper for me than doing uh you know five search results got it thank you and then velle asked uh does this service need GPU or CPU no this one doesn't because we're using azer open AI as like an API that we're hitting up uh so that's you know that's serve somewhere that has GPU nothing that we're doing requires a uh GPU we would need a GPO if we were running our own model uh but we're not running our own model for any of this all right so here you can see um that it did decide yeah it skipped a bunch of things because we'd already ined them then it decided to ingest this PDF so first it extracts the text then it splits it into sections then it uploaded the file and then it computed the embeding and then it stored it so that's just what the local script uh looks like now the other option we have is integrated vectorization this is something that's now built into as your AI search which is basically just doing everything we just saw but in the cloud hosted you know with uh you know geared for large you know large scale uses uh and it has a little it supports a little bit more um data sources right because as your AI search can be you can make indexers so it's basically making indexers and skill sets and tying them all together so you can have your indexer based off like blob storage or Cosmos Tob or whatever and as your AI search will you can run like a Cron job and basically like every five minutes it would check to see oh is there something new in Blob storage and then reindex it and only index the the things that have changed so if you use integrative vectorization you can take advantage of the indexer which can work well if you want to be able to like you know have it pulling off of some data source dynamically and only indexing the new stuff in that data source then it'll do the it'll do the data extraction and there I think it's just I think it is using document intelligence behind the scenes it'll do the chunking using a similar splitting algorithm it'll use the vectorization using your open AI deployment and then it'll index it but the big Advantage here is that you know it's doing it all it's doing it all the in the cloud and you can connect it to the indexers so that is an option to consider if you're using as your AI search uh and it's an option you can turn on in the repo that I've been showing okay so how does this code actually work okay so let's do a little code walk through just to see all right I'm actually going to go to the chat tab now because this one's more fun uh does all right so let's see actually let me go ahead and um and point put a breakpoint in let's see all right let's see can I do run okay all right I well I don't know if you're gonna want watch me do breako to can be a little slow all right I'll just I'll just put one breakpoint and then display so this is in vs code with the uh the python debugger uh I like to use the debugger when I can because I am very often debugging things so it'll it'll start up but basically what we have is a uh a front end that's in react so we have like you know this chat you know chat react stuff here and when we you know type in a question it's going to make a request to the chat API right so the SL chat and that goes to our you know python routes right so we look for Flash chat here where is it chat chat oh that went that went to the wrong place okay app.py many app.py okay okay so the frontend makes this request to the chat sends a post with the user message and the history and then sends that to our other Python and sometimes it streams it if we ask it to stream otherwise it'll just send it back all right so we should be running now so it say uh do company perks hover underwater activities it should let's see my chat retrieve yeah it should pop open the um I think it should pop open in the debugger are there libraries so you can do this from a c project yeah uh so if you want this C equivalent of this product it is this repo here so uh the other Advocates have been trying to Port the python repo into other languages you know because the python repo is so popular uh the other ones aren't as popular yet but you can make them Popular by going into them I think Jason haven't you been I feel like Jason or maybe Jason's been on the JavaScript one I saw Jason committing to one of them yeah I I committed to the JavaScript I'll put the links and so I've reviewed the different ones on my blog site I'll put the links in the chat for people okay thanks Jason okay wait it's it's being too slow so anyway so I'll just show the the thought process okay so right so you know we go into the python H and now I just wanted to show the chat tab because it's a little it's actually more complex and I think it's interesting the way and it's more complex so this is thought process for the for the chat tab so what we see is the original user query and we actually used an llm in order to turn that search query into a good keyword query and now this might seem a little silly because all we did here is just remove the question mark But in other cases it actually makes really helpful um modifications especially based off of the history that gets passed in so that you so we get the generated query and then we use that to search and then we get back the results so uh let me good an example like so then I'm going to do a followup question like more I'm just GNA more that's all I'm going to say is more let's see how it does with the um search query generation here my hope is that it takes more and turns it into a a longer a longer query okay I think it did let's see okay I'm going to click on thought process okay great so the original user query was more the generated search query was sleep strategy so this is where it's really helpful to use this llm to turn the user query into a keyword search because if we had just passed more into our search engine what would we have gotten right we would have not gotten good results at all uh so we you know we ask it like hey turn this into a search query based off of the you know convers a history so the llm realizes like oh okay a good search query would actually be sleep strategy so it's actually going to just redo the Sleep strategy from before and then it's asking the llm down here it with the llm it's saying it tells it the question where it just says more right so the llm can see like oh you wanted more sleep strategies in addition to the ones before so when you are doing you know what we call like multi-turn conversations you know conversations whatever you want to call them when you have chat history you need to keep in mind that users are going to ask follow-up questions that are not going to be as well formed as their original question and you can use the llm in order to turn their followup question into a much better keyword query for your search index okay all right so I wanted to show that so you know there's the general architecture of the code and uh now our code is using you know python for the back end and it's just using the open AI SDK with the Azure you know document search SDK it's not using any fancy llm orchestration libraries and people always ask like why aren't you using Lang chain why aren't you using this why aren't you using that well part of the reason I'm not using any of those is because everybody has a different favorite library for Python and I'd have to pick one of them and I and then that would like alienate other people right so there are many popular rag orchestration libraries and you could totally use those as well you know whichever one works for you uh so the most popular ones in the open source world are Lang chain and llama index and both of them are available in multiple languages and then from Microsoft we've got Mantic kernel which is probably the most similar to linkchain and which is that's in Python and oh I should have said net that's really what it is is net we we'll fix that python and.net I don't think it's in typescript um I obviously haven't used it very much at all and then there's also Microsoft prompt flow and that's built into a your AI studio so it works really well if you're using as your AI studio and doing other machine learning model stuff so you can totally use these libraries uh you know I tend to not use any libraries because I like to work really you know really close to the metal H because things are changing all the time and I want to be able to take advantage of everything uh but these libraries do have a lot of advantages as well because they have like documented you know ways of doing things right so llama index for example it actually has a way of indexing into postgress and doing a hybrid search uh just already built in there so they've already got that that built in so definitely if you're you know looking to do search on documents and you're looking to do it with you know you know various um you know search backends check out llama index they have all these different retrievers all these different document pars there's lots and lots of options we also have more starter repository so we just talked about actually a bunch of them so the you know this python one we're trying to Port it into other languages they're they're never going to be exactly the same and generally the python one tends to have more more features or at least more experimental features because we're just working on it so much because it's so so popular but you know we're you know the get more more eyes and more developers uh interested in these then we can you know the teams can spend more and more time on them which would be great because we would like to have full featured rag applications for all languages so if your favorite thing is C definitely check out the C repo it's actually been around for quite a while um it it came out at a similar time as the python one so it's it's got a good number of features and then there's JavaScript that team is great and there's the Java one so those are definitely options for you and if you're seeing something missing in there in there those repos that you want just file an issue and let them know uh there's also uh the one that I showed earlier which is the code that powers as your AI Studio on your data uh so you can check out the code for that uh another one is chat co-pilot and this is a demonstration of semantic colel but it is actually really really cool it has collaborative chat which I think is just really fun um so just putting there especially for those of you who are doing net which is probably quite a bit I see a question will the demonstrated code still work with a free account even in some limited way what are the limitations yeah so that's all documented on let me see H yeah this AA I'll just place this okay yeah so I did try to figure out how to get it working on free and so you can get it on free and I talk about all the limitations here although I will say there is no form of free open aai you're going to have to pay for that somewhere whether that's on open.com you could even you can actually swap out AMA I I do have support for that in the repo I think I talked about that here yeah you can't locally you could use an opening eye compatible model good question all right okay so it is now 4:30 is this the time I meant to be over Jason sorry looking for the mute button um it is but uh if you can go ahead and go through this and I don't know say 15 minutes sure yeah so I won't do like the demo like demo stuff I'll just talk about this so okay so let's see you make a rag chat application and you want to know is it good right is it high quality right you can't just write unit test I mean you should write unit test for the features but you can't just write a a you know test to say yeah it's good to go you need to evaluate the answer quality right so you want to know are these rag answers correct relative to my knowledge base right uh are they clear and understandable usually uh and are they formatted in the desired manner so if you ask for citations in square brackets with file extensions that is exactly how the citation should come through right so that's the stuff we're looking for in our rag chat app answers right so here I have three different answers that came for the same question my favorite one is the bottom one because it answered the question correctly and it has the citation in the right format and that's what I'm looking for lots and lots of things affect the quality and that's what can make it kind of hard to you know to figure out how to improve uh improve quality right so the first thing is the search right we talked about search is really really important right so what search engine are you using are you cleaning up your query like what are your actual queries come in that they look look like are you using hybrid search um any other search options you're using how big is your data like how large are your chunks uh you know are do they overlap with each other how many search results are you returning these are all things that can affect the results and then finally we you know we pass things into the large language model so there the system prompt can affect things especially like whether it uses citations or not uh the language of the prompt if you're doing something not in English you should actually write your prompt in that Lang Ang instead if you wanted to Output in that language uh how much conversation history you're passing in which model you're using has a huge effect generally qd4 is much it's like really it's just really good at things so um you know model definitely has effect I use gbd 35 a lot but I've definitely seen places where gbd4 was just nicer better more adherent to the instructions I'll say temperature can affect things all these parameters right so you know what do you do like first thing you do is just do manual experimentation right so in our you know in this repo um we can experiment using developer settings so we can you know override the system prompt here we can change the temperature we can change our search results we can do all these other you know changes to the search results and so we could just do manual experimentation just to get a feel for what changes when we change these settings so that's a good thing just to get some more intuition for how things are working but then what you really want to do is to do an automated evaluation once you think you've found settings that work well right if you if your manual tests are working well and you're like okay I think this is working well then you want to do an automated evaluation on a lot of um question answer periods to figure out is it really working well right across lots of questions and answers so the first thing you're going to need is ground truuth data that ground truth Theta is the ideal answer for a particular question so you generally want at least 200 ground truth question answer pairs uh you can use an llm to generate these question answer pairs but you should definitely curate them manually go through them make sure they're actually legit and um you know and grounded uh I have this script here okay so I have this repo AI rag chat evaluator let me uh P put this in the chat which everyone it'll let me click on there we go so this repo has a bunch of tools that I've made to make it easier to evaluate a rag chat app so one of them is a tool that generates ground truth data and it uses this as your AI uh evaluate SDK so this will generate data based off an Azure AI search index uh you could change it if you have a database instead you could just modify it uh and so you could you know generate that data look through it curate it uh if you've got your rag chat live you could add like thumbs up and thumbs down buttons to you know actual answers to find out which answers aren't working well for people and then you should add those questions with their correct answer to your ground truth data to make sure that you're evaluating according to the kind of questions and answers that users are getting now once you've got that ground truth data the next step is to evaluate right so the way we're actually going to do evaluation is hit up our you know our our current version of the app uh whether that's local or deployed or whatever but hit up the version we want to test with the question from the ground to Theta get the the you know the new answer and then we send you know the the new question you know the the current um answer for that question along with the ground truth answer for that question and we actually send that to gb4 and we ask gp4 to rate it and we say hey gbd4 from one to five how grounded was this answer from 1 to five how coherent was the answer from 1 to five how relevant is this answer and our prompt is a lot longer than that but the general idea is that we use GPT in order to evaluate GPT which is a funny thing to do but it actually does work quite well and there's a lot of research around it uh we can also do other calculate other metrics as well like look at how long it was look at whether it had citations look at whether the citation in the new answer match the citation in the ground truth answer that's actually my favorite metric and that we can just do you know with our python so then after we run those evaluations we can compare them so you can see here's a bunch of comparisons I was doing doing where I was playing around with different prompts uh there's lots of things you can change but in this case I was changing prompts and so I was looking at you know what was the groundedness and coherence and then you know which of them had citations because that's really important to me so I can look across the runs to get an idea for how well things are working and uh it can also compare answers between runs and say like okay well what was the answer like for this run versus this other run to try and get a feel for how things changed and yeah I'm going very fast through this part so as Jason said I do have a um a whole video about evaluating chat app and he linked to it there so thank you Jason okay so that's evaluation I definitely recommend doing it because you want to make sure you're putting out a you know a quality application that isn't making stuff up uh and the the uh you asked about hallucinations earlier I have a recent I think my most recent blog post is actually about it yeah so I added a metric specifically well this isn't quite hallucinations this is just making sure the app says it doesn't know if something's just completely off topic for it that it shouldn't know so it's slightly different from huc naations and something it should know but this is also something that's really interesting that I've been digging into lately okay and just a few slides about observability for rag chat apps especially on azzer so the first thing you might want to do is integrate with Az your monitor because that's our standard way of you know uh tracing our apps on azir so you can use instrumentation libraries to send uh the open AI traces to application insights so this is how we do it in Python using this Library here and then when we look in Azure monitor we can you know see the nice little water flow and then click on the chat request and we can see like okay you know here's the parameters here's the actual prompt here's the question so we can actually see all that in as your monitor uh it's not perfect and we're trying to make some improvements to it but it's pretty good another thing you could use is an open source tool called Lane fuse and I really like it it is a um a you know it's a tool specifically for open Ai observability and it's also got evaluations and stuff and it's just a really nice UI uh so you know you have to um you know you can deploy that UI to to aser and I have a repo here where you can do that and then you just uh you know bring in their tracing like this and then you get this really really cool UI that's very focused on open AI uh usage so it's very helpful to see all your calls and see all the tokens they use and see how slow your gbd3 calls are versus your gbd4 and all that stuff and okay this is my final slide so uh yeah so if you're interested in this you can try creating a rag app uh you know and I pointed to the free instructions here so so that you're not spending money doing that and you can read about the limitations air you can try out the evaluation tools as well uh anytime you try these please do raise any issues I do see everything that comes in I can't always respond the the day that I see them but I try to respond to everything so that we can make things better and generally I recommend sharing your learning so like Jason has all these blog posts where he reviewed stuff like so if any of you blog or tweet or whatever please share what you learn about building a rag app because I think still early days for this and we still all have a lot to learn in terms of best practices and that's what I have awesome that was awesome any uh any other questions out there I think you got it all that was great that was uh that was uh really good I I've really gotten into the r stuff so the more that I uh see people present it who really know this stuff is it's just fantastic to be there so I think this was probably the most interactive session we've had in a very long time yeah you all had great great questions I love getting the questions because usually there's something afterwards where I'm like oh I really got to figure out a good answer to that question and then it gives me an excuse to dig into something I don't think they asked too many things you didn't have an answer to I have to try harder next time no I gota like export the chat awesome well I I think that's it Bill's going toh do a Hands-On thing after um so yeah all right well thank you everyone thanks yeah thanks pam pam thank you Pamela that was fantastic so quick question great can we get the recording of this session is it possible to share the recording yeah um it will be on our Boston Azure YouTube channel it's youtube.com Boston Azure isn't that right bill I think it's the full boss word yeah and um it'll probably be up there late tonight or B tomorrow morning sometime okay so so from meet up I should be able to access that right and no well we I believe we do have a link to the YouTube channel in the Meetup description so you should be able to find it yeah okay thank you cool all right okay bye everyone have a good night thank you thank you Bill are you gonna take over can you yeah I just I flipped on um sharing and it's I got to go set a permissions sorry folks give me a second let's see I have here um so let me share my screen why bills getting his duck R here so this is where we're at okay cool Phil's there Bill's got it cool um yeah so you can get the recording at uh youtube.com Boston Azure that'll have pelis stuff and um I'm going to transition um I can only share my um my web browser without having to configure something my apologies so I have a couple of slides I was going to share I'm going to speak to those um what I'm going to show here is a um we I'm I'm going to ask you to create a simple python app or a simple uh C app um per you know instructions I um I basically think you know how to do that that and I'm going to ask you to basically do that and then I have some code to uh to slam in to make it easier the goal of the workshop this mini Workshop is to see um if you've been doing this for a while the this might not be for you but if you're new to this and you you don't really know what the code looks like um my hope is that at the end of this you'll have the aha moment of wow I can do this in about 10 lines of meaningful code not everything Pamela did but you can actually create a program that can do something using the llm that a programmer just before this whole llm thing came on it was impossible to do no programmer was able to do this in you know before this this is a remarkable um change in just the way uh the way the the tools that we have access to as um as developers so I am sharing let's see I'm sharing in the chat two links um this is um okay so I just shared two links um I was also going to go through and show you how to do this that's going to be a little trickier now because like I'm having trouble sharing Beyond just my web browser uh but here's the the idea if you're a a python developer um go follow the python link you know which is which brings you here and if you're a net developer cop developer follow that one and there are directions at the top of each file basically create a folder and then um do do a couple of steps that are normal things to do if you're a python or. net developer and then replace or create the the file if it's cop you're going to replace program.cs with um with this file tells you to do that here um and if you're a python developer you're just going to create a file called hello ai. py and you know do the stuff mentioned here so these two links are in the teams chat um and after you do these few steps up here there's a there's a challenge that um um that you know once you have it running I'll I'll I'll speak to that so I I hope this only takes you know five minutes for folks or maybe 10 minutes for folks to to get running but if you have questions come off you know go on audio or uh post up in the um in the chat bill you want me to share anything and you can talk to it I had it all set up on my um uh it a command lines I you know I have windows open that um maybe in the metime I can I can see if I can even extend my permissions um I so no it'll be hard to give you something to to share Jason unless you wanted to follow along the lab and do that live okay I mean I can do that too meantime I'm going to go see if I can enhance my permissions here [Music] m I share mine until you get yours going Bill see this guy so let me take this here so got empty folder do this gu e all right clob or the content just copy and paste the code all right so here paste this guy save okay done it run okay that worked perfectly oh an interesting fact that fast cool if you um so I'll wait I'll wait till to proceed um I'll I'll talk a little more once everybody has this at least running I can do it so I try to put this code in the collab andun trying to run it think I'm just having some issues like configure environment for python no module name no open some those kind of er maybe I need to do few things before so you did the python one yeah I did the Google cab and did the python Google collab I'm not sure what that is yeah collab is the environment where you can run the python Cod um yeah I've heard of it I've never used it okay so let's try the python one where's the link that python [Music] say mac thing um or Linux you know uh thing my mistake you want to run activate um so it's probably just B activate on window po I can do it in code if I don't have any luck let me look that one up that's I'm kind of handicapped when it comes to python so yeah this is the let me start over with this close CD MK oh it's activate do PS1 I think but it's do it well actually you you figured out um or or if you're onto something yeah I'm just going to do this so if I go here yeah I do dude trying to type sometimes challenge over okay so now close this welcome window so we'll take this what do you want this to be hello AI well you um there's more to the file than that unfortunately is there okay it's it's above the comments too so I would just do a control a and take the whole thing in all copy paste all right see what we got here so here we have this deployment okay so that's that so now we basically I think I already have these you might not need to do anything except just run it yeah actually it's true um let me just try to run it I could have done a notebook there a there we go different one um and this a uh yes George contributed a scripts activate command which maybe is the one that you need for po shell I probably should have opened the mic let you guys struggle through that but yeah just oh good I thought it was quick on the chat I was like oh well I know C SHP guys anyway so yeah me too try python I can fumble around and get most things working but slowly getting there okay so anybody um anybody have this working besides Jason either one and you hit the uh you keep score if you want to check the I put put a comment in there you could respond to you could click the check box no obligation but trying to have some way to track how people are doing all you got three of them now what do they look like next to each other python anybody have any struggles they want to vocalize who's got it you know my my struggle with these is always the cost right whenever you do any of these cloud computing especially these services like this I'm always very leery I mean I actually I'm working on a project right now where it's all local so our local CPU and GPU I got a local llama 2 model that I'm using um you know the stuff with an Azure is really awesome but do you have any indication or you know setting up like workflows like this the cost involved I mean I well th this one is um uh actually actually that's a good question I don't have a readymade answer for that uh my perception is this is not expensive um but um but if you add uh like the Azure AI search um you know the the it goes up to minimum of some number $100 a month okay uh you add a vector database you know the same thing this is using um if I could show my da uh Graphics here I made a beauti I had uh uh do 3 make me a nice uh uh visual um but um you know this is using the the tip of the tip of the iceberg and um this this just you're not using the the heavyweight uh tools yet not using um um you know the the responsible AI um uh services to you know make sure your your data is safe the the the safety uh service and so forth yeah you're you're in the the cheap Zone here there are different approaches how you can minimize the cost obviously you can drive it to zero especially as you mentioned when you work with Cloud technologes um but you know you're using llama deployed on um your local machine you can deploy the servers so you you kind of escaping that cloud approach Al together so making it a little cheaper and then caching so you can cach the responses so you are not calling the service all the time if you are using something like Azure open AI um or maybe store most popular answers somewhere in um like a a database or something like that that's interesting I never thought about caching the responses that come back yeah so that's that's one uh that it depends what you're building right um so that that's one of the approaches how you can minimize the cost um and then you know something like rag you don't have in in some cases you don't have to call um Azure open AR or something like that at all so if you're um doing the search and then you're getting decent response from the search then you can escape that second and stop of calling them um llm or ji service alog together that's interesting I never thought you got me thinking about maybe cashing the common responses okay thank you no problem so I'm going to grab so does anybody have any struggles any struggles out there who's still here is is doing good so I'm going to grab the screen back and walk through a couple things here so um do you saw the output so I I assume I assume you can see your output and it basically shows um a prompt that appears I'm I'm let me zoom in on this code here that the code is essentially the same between Python and uh C so um this is the prompt that you're that is being sent to the um under the hood the open AI uh llm large language model and and this is the part that's you know magic it's producing a response that is uh unique text uh that you know a response that the kind of a response that was impossible before this um LM Revolution uh started year and a half ago that's like just amazing and if and um one of the things that I claim is if you look at the response there's something wrong wrong about it and I didn't want to spoil it give folks a chance to look at the response but when you look at the the response um um there's there's a lack of um grounding or or the opposite the other side of the grounding coin is uh hallucination of sorts happening in the response so um I'm suggesting that um a hint to help you U solve for that don't want to take a huge amount of time here but that that's that's the point and um this response setting the number of tokens we can talk about these um April 20th setting a prompt um scrolling up here creating this Azure open AI uh you know object and the rest of this is kind of boiler plate stuff um this is like 10 lines of code that that you're now able to do something that no software engineer could do for the first 75 years of the computer industry let's go back here how we doing um it ah so yay somebody declared a date buug and they fixed it good two people a couple people fix a date buug so um the the date buug if you haven't figured it out by now is actually Jason would you mind sharing again and showing um showing your uh your output sure just SEC let's see share this one the C output was here do you want to see the code or the output the output okay so H this was the python code either one is the output is down here and then up here is the um other one up at the top thank you so if folks look at that you're seeing similar stuff you know what today's date is because it's output as a debug message um and the prompt asks for something from some world history event from today's date and and it gives you one except with great confidence uh on March 18th 18 uh 1965 uh Soviets walked in space um but uh that great confidence is uh untethered from reality because that didn't happen on this day in history it happened on a different day and that's uh that's your llm hallucinating returning a confident result when it has no business doing that and the way to ground it is to do um basically rag this is this is exactly what Pamela was talking about except that with the rag um here it's a much simpler rag where the augmentation is simply the date so you don't have to make a network call to get the the uh the current current date you can just make you know call a a python library or a cop library to uh to get that then weave it into the prompt uh I'll leave that to your imagination how to do that you know on this day in history you know assuming this day is you know X or something like that would do fine which is date whatever you know choose your style and see if you get uh consistent results and if you do then that's that's the win and um and and that was the the goal is to uh to give you the the hands-on experience hopefully a couple of a house quiet group thank you Tony see I go here and if you've accomplished the the goal here and you want to try something different you can instead of just uh prompt engineering or or ragging to to get the date inserted in the in the prompt itself so that the respones um more accurate better grounded uh you can also say something like uh and return the results in French or translate the results to German uh you know append that to your prompt and see if that works I I tested it earlier with mandarin and it was happy to do that and it also worked with pig latin um Mandarin didn't display on my my son was doing some testing on me um uh and um he realized that Mandarin didn't display well in some contexts on his windows box mine is I I'm using a Mac didn't have any trouble for me so your mileage if it's a different alphabet uh your mileage May VAR my response looks gibberish how many uh oh you didn't um no you got 250 tokens used all the tokens it ran out tokens I changed the temperature to two I thought the temperature only went up to one I goes two those people between one Veronica help what's going on yeah temperature can be more than one that one's a little better Al the the one above used all 250 tokens this one only used 50 so it hit some cognitive limit there when it ran out of tokens maybe I thought temperature was a was like a zero to one real number I guess you can scale it if way I know I've seen it from negatives to positives and stuff too but one concept going into junk at 1.7 how about 1.6 1.5 was good it's the top P that's you would want that's also gibberish starts off good it's getting tired it's passes bedtime so um SAR how to fix the date so the the the challenge is that um oops somebody um weighed in Tony so um Yep this this looks like a python solution I don't know which one you're using sadar yes I'm using B okay so T Tony's solution how to do it basically you just want to uh Wordsmith the prompt to also in normal English uh declare what today's date is and that becomes part of the information that the llm uses and again that's a very simple form of rag I see I see Conor so the initial one you're not actually passing what today is on purpose that's the bug that's the the challenge the the my My Hope was people would notice the you know discover for themselves that there's a hallucination there that you ask for today to something today and it gives you something you know in September and then something in June and you know and so forth because it has no concept of the day that's a hallucination then you ground it using rag you know rag light I guess simple Rag by just inserting the date like that let's up with that Jason you you were playing with the uh the temperature can you explain to the to the room what what that um what that means yeah so the temperature is what you see most demos change was it I just passed it fine temperature okay so I guess this is the top pie H interesting um the temp here so this is a temperature it's a float it's between zero and two but it typically uh is explained as in it is a spectrum of creativity that you want in the response so a lot of rag Solutions are geared to zero because that you don't want creativity but if you're say doing um writing of some sort say fiction writing you would want to get gaug it closer to two the defaults one right middle of the road a lot of the rag systems will be zero or up to maybe3 but not much more than that that I've seen anyways not saying they all are so this is the one I want nuc nucleus sampling factor is a different one and yeah so here if it's set then temperature is kind of um ignored it's going to be one of the other but this one is geared more toward the sampling here and it's between 0 and one sorry what was the question where's it at on the bottom down here yeah so for co-pilot or chat the creative balanced precise is that basically the temperature yeah sense and then the co-pilot in like word and Outlook have the same idea don't they I don't have those running so I've only seen demos yeah I don't have those either but they should they they're all based on um the same approach um if Pam was sh in both top PE and then the temperature um most people just use temperature um but if you want it extra creative you can adjust both top p and um the temperature cool yeah you kind of just have to play with it see what works for [Music] you I'll mention that in the code um there's a comment up the the top uh that says um do not do this in real code it's a terrible idea uh because I in order to simplify this mini Workshop rather than externalizing the the the variables like the API key value um I burned them in so I'm going to roll those I'm going to roll that key at some point after after we hang up so your your onepage uh AI machine will will uh cease to work at that point good idea actually prepared um a a a bash version a Powershell version a command version and I struggled with it and I ended up I still have them but I ended up uh doing this just to just to simplify and uh I I don't know how much it simplified it but probably at least a little well does anybody um um than thank you all for for attending I hope this was uh useful for those of you newer to the to the uh Azure AI world uh does anybody have any uh closing questions yeah this is great thank you and H how many tokens does the open asure support like um I know normally they would charge right but in this case um is it is it free for to use a or how how long we can use this if I keep running it well I'm going to roll the credentials after this uh call ends so we'll stop working I I I see you know five minutes uh it actually isn't free uh but it's not terribly expensive this is tied to my personal credit card okay but it's it's not like it's not a lot of money to do what we did that's if you look in the comments you'll see I put a comment to not raise the max tokens above 250 so you could go you know everybody could go crazy and try to do a lot of stuff um but you know this felt like a this is experimental for for myself and uh roner and Jason and because we have to also figure out how to do what we going to whatever we're going to do on the Azure um day you know the full day event on April 20th how do we light up this for everybody because it's not an easily available service so um I'll check the the numbers later and see if this cost me more than a dollar I I don't expect it it will though I'd expect it to be pennies if anything so I hope that's that's the best answer I have at the moment oh thank you I really appreciate that I I did run it some sometimes so didn't realize that's the case sorry about that sorry I I think the volumes were at um and it's kind of in 10 people left or something Veronica or Jason do either of you have any ideas how to like what this exercise that we did for this um mini Workshop um given parameters that we were using and the the volume that we were pumping into that would you expect that to not even be a penny or any idea it should be really cheap I'd say less than a quarter um more than a penny though all right yeah uh what is actually hold on I can bring up the pricing open AI pricing it's um it's like it's kind of like reminds me of storage was like per thousand type stuff uh so is it gp4 or 35 it's a 35 turbo5 tur of course this isn't the pricing page uh let's see this is the this doesn't tell me the model pricing uh that's for open a I um think Azure is charging something on top of it are they so Azure open AI model pricing now this one  this thing can't see because of the bar at the top this guy okay so per thousand token so for heading to 50 well see these input output but it does add up but not a lot I mean th tokens if you cap it at 250 and you hit the cap every time four of those you're at this much so time up by you'd have to have like a 100 before you'd hit five I I'll check the uh I I actually have the cost analysis window popped up on the Azure side and I'll say I don't know how much latency there is before it catches up it takes a while yeah I use that in my demo the um but I mean this is a a good thing to point out I mean look how much price difference there is between GPT T4 with 32k and turbo I mean it's huge this is a scent one penny for 100 or a th tokens out and then so you could easily yeah right but I was watching a video actually a couple videos from the guys over Lang chain and they've been doing testing on the context window basically finding I think they call it needle and hay stack so they put like I forget how many needles they were doing needles so they put it something in the context window that they're passing they put were putting like one to seven they did multiple tests to see how often it would find those four things and it other test showed that the larger the window was the more often it paid attention to the end of the context it would sometimes lose things at the beginning of the context so yeah it's all interesting stuff anyways so you did mention that you are going to have some kind of a workshop in the future like is is it to uh open to everyone sure I can answer that sure yeah it's open to everyone so we're this is a um an event that Veronica and Jason and myself have put on a bunch of times over the years where we've been doing it for more than 10 years where we have an all day uh free usually vendor sponsored so we can do food and and such um at nerd in Cambridge Massachusetts uh where uh you show up with a laptop or you know we we'll we'll uh we'll publish a page that describes all this but show up at usually it's around 8:30 or 9 or something and at the at the site and you bring your laptop like I mentioned and we code we do stuff there's a usually a an interwoven set of talks like like the Pamela talk that you just heard um as a you know one example there might be a talk like that and then there'd be um a hands-on experience after that to take advantage of the information there and then you know uh you repeat that three four five six times during the day you know new topic new lab new topic new live that kind of thing and at the end of the day um you uh you leave presumably Having learned a bunch more about Azure you know this is the Boston Azure group but we're um um we're we're expecting to have a heavy AI emphasis on um on this particular workshop on April 20th well that's great I would love to I'm in New Jersey but I would love to come there and join it love to have you great we so watch the Meetup space there'll be a sign up um soon um we like uh we we only found out about this uh quite this is quite recently this late breaking news so we have a couple of details to square away now that we know we have space but we'll we'll get a sign up form out there and um if you should you know assuming you get Meetup notifications that should um should let you know great thank you so much yeah welcome have you been to any in-person uh Boston Azure or North Boston Azure events no no I I haven't come to any Boston a events but I did go to some in New York New York City ah very cool oh welcome thank you any other um questions out there folks we let everybody go sounds good have a good night okay thanks for staying late all uh I'm tired you are too good night everybody Forest gum what's he say I'm tired I'm gonna go home now that's how I feel that's what I'm gonna two and a half hours this is great thank you guys thank you very much take care thanks everybody thank you okay 