{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Property Graph RAG in C#\n",
    "\n",
    "Currently this notebook uses the following resources:\n",
    "* Azure Open AI\n",
    "* Neo4j\n",
    "\n",
    "If there is enough interest, I can add the changes needed to just use OpenAI - so if this is something you'd like, let me know on twitter @haleyjason or open an issue on github.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Add the references and using statements used in the rest of the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Azure, 1.0.6</span></li><li><span>Azure.AI.OpenAI, 2.0.0-beta.2</span></li><li><span>Azure.Identity, 1.13.0-beta.1</span></li><li><span>dotenv.net, 3.2.0</span></li><li><span>Microsoft.DotNet.Interactive.AIUtilities, 1.0.0-beta.24229.4</span></li><li><span>Microsoft.ML.Tokenizers, 0.22.0-preview.24378.1</span></li><li><span>Microsoft.SemanticKernel.Core, 1.16.2</span></li><li><span>Neo4j.Driver, 5.23.0</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r \"nuget: Azure.AI.OpenAI, *-*\"\n",
    "#r \"nuget: Azure, *-*\"\n",
    "#r \"nuget: Azure.Identity, *-*\"\n",
    "#r \"nuget: dotenv.net, *-*\"\n",
    "#r \"nuget: Microsoft.DotNet.Interactive.AIUtilities, *-*\"\n",
    "#r \"nuget: Microsoft.ML.Tokenizers, *-*\"\n",
    "#r \"nuget: Microsoft.SemanticKernel.Core, *-*\"\n",
    "#r \"nuget: Neo4j.Driver, *-*\"\n",
    "\n",
    "using Microsoft.DotNet.Interactive;\n",
    "using Microsoft.DotNet.Interactive.AIUtilities;\n",
    "using dotenv.net;\n",
    "using Azure.AI.OpenAI;\n",
    "using Azure;\n",
    "using Azure.Identity;\n",
    "using OpenAI.Chat;\n",
    "using System;\n",
    "using System.Text.Json;\n",
    "using System.Text.Json.Serialization;\n",
    "using System.Text.RegularExpressions;\n",
    "using System.IO;\n",
    "using Microsoft.SemanticKernel.Text;\n",
    "using Microsoft.ML.Tokenizers;\n",
    "using Neo4j.Driver;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the environment variables. **The notebook assumes you have a .env file** with the following contents:\n",
    "\n",
    "```cmd\n",
    "AZURE_OPENAI_ENDPOINT=\"<you azure open ai endpoint>\"\n",
    "AZURE_OPENAI_RESOURCE=\"<you azure open ai resource name>\"\n",
    "AZURE_OPENAI_API_KEY=\"<your azure open ai key>\"\n",
    "AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=\"<name of your embeddings deployment>\"\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT=\"<name of your chat deployment>\"\n",
    "\n",
    "NEO4J_URI=\"neo4j://localhost:7687\"\n",
    "NEO4J_USER=\"<neo4 user name>\"\n",
    "NEO4J_PASSWORD=\"<neo4j user password>\"\n",
    "NEO4J_DATABASE=\"<name of you neo4j database>\",\n",
    "```\n",
    "\n",
    "> Note: I did my testing using text-embedding-ada-002 for embeddings and gpt-4o for the chat service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "DotEnv.Load();\n",
    "\n",
    "var envVars = DotEnv.Read();\n",
    "\n",
    "AzureOpenAIClient client = new(new Uri(envVars[\"AZURE_OPENAI_ENDPOINT\"]), \n",
    "    new AzureKeyCredential(envVars[\"AZURE_OPENAI_API_KEY\"]));\n",
    "\n",
    "var embeddings = envVars[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"];\n",
    "var llm = envVars[\"AZURE_OPENAI_CHAT_DEPLOYMENT\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE: You may want to skip to the Neo4j Connection step to make sure your environment variables settings are complete before continuing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ingestion phase is broken up in to the following steps, which should allow for some experimentation with the different steps:\n",
    "1. define the data structures used in extracting the entities and populating the Neo4j database\n",
    "2. call the LLM to extract entities\n",
    "3. process the results into a unique list of entities and their relationships\n",
    "4. generate the cypher to populate Neo4j\n",
    "5. populate the Neo4j database\n",
    "6. create and populate vector and full text indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "public record DocunentMetadata(string id, string source);\n",
    "public record ChunkMetadata(string id, string name, int sequence, string documentId, string text);\n",
    "public record TripletRow(string head, string head_type, string relation, string tail, string tail_type);\n",
    "public class EntityMetadata\n",
    "{\n",
    "    public string name { get; set; }\n",
    "    public string type { get; set; }\n",
    "    public string id { get; set; }\n",
    "    public string text { get; set; }\n",
    "    public Dictionary<string, ChunkMetadata> mentionedInChunks {get; set;} = new Dictionary<string, ChunkMetadata>();\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity extraction\n",
    "\n",
    "This step is where the LLM takes the chunks of text and extracts up to 10 entities per chunk\n",
    "\n",
    "Steps include:\n",
    "* Break the data file into chunks using 500 tokens and 100 token overlap as limits\n",
    "* Provide some default entities and relation types for the prompt to use in directing the LLM in extracting the entities (extration works best if you customize this to match your data file contents)\n",
    "* Loop through all the chunks calling the LLM for each chunk **(Warning: this can get expensive - so change the ```paragraphs.Count``` limit to 1 or 2 until you are happy with your results)**\n",
    "* Parse each JSON result form the LLM calls and keep the ```chunks``` variable for later post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "ChatClient chatClient = client.GetChatClient(llm);\n",
    "string fileName = \"data/summaries.txt\";\n",
    "string fileText = File.ReadAllText(fileName);\n",
    "\n",
    "DocunentMetadata documentMetatdata = new (Guid.NewGuid().ToString(\"N\"), fileName);\n",
    "\n",
    "var tokenizer = TiktokenTokenizer.CreateForModel(\"gpt-4o\");\n",
    "#pragma warning disable SKEXP0050\n",
    "var lines = TextChunker.SplitPlainTextLines(fileText, 500, text => tokenizer.CountTokens(text));\n",
    "var paragraphs = TextChunker.SplitPlainTextParagraphs(lines, 500, 100, null, text => tokenizer.CountTokens(text));\n",
    "\n",
    "string entityTypes = \"BLOG_POST,PRESENTATION,EVENT,ORGANIZATION,PERSON,PLACE,TECHNOLOGY,SOFTWARE_SYSTEM,REVIEW,ACTION\";\n",
    "string relationTypes = \"WRITTEN_BY,PRESENTED_BY,PART_OF,LOCATED_IN,LIVES_IN,TRAVELED_TO\";\n",
    "\n",
    "Dictionary<ChunkMetadata, List<TripletRow>> chunks = new Dictionary<ChunkMetadata, List<TripletRow>>();\n",
    "int maxTripletsPerChunk = 10;\n",
    "for (int i = 0; i < paragraphs.Count; i++)\n",
    "{\n",
    "    string text = paragraphs[i];\n",
    "\n",
    "    ChunkMetadata chunkMetadata = new (Guid.NewGuid().ToString(\"N\"), $\"DocumentChunk{i}\", i, documentMetatdata.id, text);\n",
    "\n",
    "\tstring prompt =  $@\"Please extract up to {maxTripletsPerChunk} knowledge triplets from the provied text.\n",
    "    Each triplet should be in the form of (head, relation, tail) with their respective types.\n",
    "    ######################\n",
    "    ONTOLOGY:\n",
    "    Entity Types: {entityTypes}\n",
    "    Relation Types: {relationTypes}\n",
    "    \n",
    "    Use these entity types and relation types as a starting point, introduce new types if necessary based on the context.\n",
    "    \n",
    "    GUIDELINES:\n",
    "    - Output in JSON format: [{{\"\"head\"\": \"\"\"\", \"\"head_type\"\": \"\"\"\", \"\"relation\"\": \"\"\"\", \"\"tail\"\": \"\"\"\", \"\"tail_type\"\": \"\"\"\"}}]\n",
    "    - Use the full form for entities (ie., 'Artificial Intelligence' instead of 'AI')\n",
    "    - Keep entities and relation names concise (3-5 words max)\n",
    "    - Break down complex phrases into multiple triplets\n",
    "    - Ensure the knowledge graph is coherent and easily understandable\n",
    "    ######################\n",
    "    EXAMPLE:\n",
    "    Text: Jason Haley, chief engineer of Jason Haley Consulting, wrote a new blog post titled 'Study Notes: GraphRAG - Property Graphs' about creating a property graph RAG system using Semantic Kernel. \n",
    "    Output:\n",
    "    [{{\"\"head\"\": \"\"Jason Haley\"\", \"\"head_type\"\": \"\"PERSON\"\", \"\"relation\"\": \"\"WORKS_FOR\"\", \"\"tail\"\": \"\"Jason Haley Consulting\"\", \"\"tail_type\"\": \"\"COMPANY\"\"}},\n",
    "    {{\"\"head\"\": \"\"Study Notes: GraphRAG - Property Grids\"\", \"\"head_type\"\": \"\"BLOG_POST\"\", \"\"relation\"\": \"\"WRITTEN_BY\"\", \"\"tail\"\": \"\"Jason Haley\"\", \"\"tail_type\"\": \"\"PERSON\"\"}},\n",
    "    {{\"\"head\"\": \"\"Study Notes: GraphRAG - Property Grids\"\", \"\"head_type\"\": \"\"BLOG_POST\"\", \"\"relation\"\": \"\"POSTED_ON\"\", \"\"tail\"\": \"\"Monday August 5, 2024\"\", \"\"tail_type\"\": \"\"PERSON\"\"}},\n",
    "    {{\"\"head\"\": \"\"property grid RAG system\"\", \"\"head_type\"\": \"\"SOFTWARE_SYSTEM\"\", \"\"relation\"\": \"\"USES\"\", \"\"tail\"\": \"\"Semantic Kernel\"\", \"\"tail_type\"\": \"\"TECHNOLOGY\"\"}}]\n",
    "    ######################\n",
    "    Text: {text}\n",
    "    ######################\n",
    "    Output:\";\n",
    "\n",
    "\tChatCompletion completion = chatClient.CompleteChat(\n",
    "    \t[\n",
    "        \tnew UserChatMessage(prompt),\n",
    "    \t]);\n",
    "\n",
    "\tConsole.WriteLine($\"{completion.Role}: {completion.Content[0].Text}\");\n",
    "    List<TripletRow> rows =  JsonSerializer.Deserialize<List<TripletRow>>(completion.Content[0].Text.Replace(\"```json\", \"\").Replace(\"```\",\"\").Replace(\"'\", \"\").Trim());\n",
    "    \n",
    "    chunks.Add(chunkMetadata, rows);\n",
    "}\n",
    "\n",
    "Console.WriteLine($\"Number of chunks: {chunks.Count}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility Methods**\n",
    "\n",
    "Create some utility methods for parsing the LLM results into data structures we can use to generate cypher for Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "public class Utilities\n",
    "{    \n",
    "    public static EntityMetadata PopulateEntityMetadata(ChunkMetadata chunkMetadata, TripletRow triplet, EntityMetadata entityMetadata, bool isHead = true)\n",
    "    {\n",
    "        entityMetadata.id = Guid.NewGuid().ToString(\"N\");\n",
    "\n",
    "        if (isHead)\n",
    "        {\n",
    "            entityMetadata.name = CreateName(triplet.head);\n",
    "            entityMetadata.type = triplet.head_type;\n",
    "            entityMetadata.text = triplet.head;\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            entityMetadata.name = CreateName(triplet.tail);\n",
    "            entityMetadata.type = triplet.tail_type;\n",
    "            entityMetadata.text = triplet.tail;\n",
    "        }\n",
    "\n",
    "        entityMetadata.mentionedInChunks.Add(chunkMetadata.id, chunkMetadata);\n",
    "        \n",
    "        return entityMetadata;\n",
    "    }\n",
    "\n",
    "    public static string CreateName(string text)\n",
    "    {\n",
    "        if (string.IsNullOrEmpty(text))\n",
    "            return text;\n",
    "\n",
    "        // Split the text into words\n",
    "        string[] words = text.Split(new[] { ' ', '-', '_' }, StringSplitOptions.RemoveEmptyEntries);\n",
    "\n",
    "        StringBuilder nameText = new StringBuilder();\n",
    "        \n",
    "        foreach (string word in words)\n",
    "        {\n",
    "            // Capitalize the first letter and make the rest lowercase\n",
    "            var lword = word;\n",
    "            if (char.IsDigit(word[0]))\n",
    "            {\n",
    "                lword = \"_\" + word;\n",
    "            }\n",
    "\n",
    "            nameText.Append(lword.ToLower());\n",
    "        }\n",
    "        return Regex.Replace(nameText.ToString(), \"[^a-zA-Z0-9_]\", \"\");\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop throug the LLM results and create a dictionary of the entitites. In order to create a relation from each entity to the document chunk it was extracted from we also keep a mentionedInChunk dictionary (this could be a 1 to many relationship)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique entity count: 103\r\n"
     ]
    }
   ],
   "source": [
    "Dictionary<string,EntityMetadata> entities = new Dictionary<string,EntityMetadata>();\n",
    "\n",
    "foreach (ChunkMetadata key in chunks.Keys)\n",
    "{\n",
    "    List<TripletRow> triplets = chunks[key];\n",
    "    foreach (var triplet in triplets)\n",
    "    {\n",
    "        EntityMetadata entity;\n",
    "        string pcHead = Utilities.CreateName(triplet.head);\n",
    "        if (entities.ContainsKey(pcHead)) \n",
    "        {\n",
    "            entity = entities[pcHead];\n",
    "            if (!entity.mentionedInChunks.ContainsKey(key.id))\n",
    "            {\n",
    "                entity.mentionedInChunks.Add(key.id, key);\n",
    "            }\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            entity = new EntityMetadata();   \n",
    "            entities.Add(pcHead, Utilities.PopulateEntityMetadata(key, triplet, entity, true));\n",
    "        }      \n",
    "\n",
    "        string pcTail = Utilities.CreateName(triplet.tail);\n",
    "        if (entities.ContainsKey(pcTail)) \n",
    "        {\n",
    "            entity = entities[pcTail];\n",
    "            if (!entity.mentionedInChunks.ContainsKey(key.id))\n",
    "            {\n",
    "                entity.mentionedInChunks.Add(key.id, key);\n",
    "            }\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            entity = new EntityMetadata();   \n",
    "            entities.Add(pcTail, Utilities.PopulateEntityMetadata(key, triplet, entity, false));\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "Console.WriteLine($\"Unique entity count: {entities.Count}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see the entities and list of which document chunks they were extracted from, you can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "foreach(var key in entities.Keys)\n",
    "{\n",
    "    var e = entities[key];\n",
    "    Console.WriteLine($\"{key} Mentioned In {e.mentionedInChunks.Count} chunks\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is all about generatint the cypher to populate the entities extracted by the LLM into Neo4j.\n",
    "\n",
    "> NOTE: If you are using a different graph database, this is the first step you'll probably need to start changing things\n",
    "\n",
    "The results of this step look something like this:\n",
    "```cypher\n",
    "MERGE (Document1:DOCUMENT { id: '54e9916c99ef4459ae8eabb227a5c341', name:'Document1', type:'DOCUMENT', source: 'data/summaries.txt'})\n",
    "MERGE (DocumentChunk0:DOCUMENT_CHUNK { id: '8dcf15992ced4c8ba77e9dd6f9372241', name: 'DocumentChunk0', type: 'DOCUMENT_CHUNK', documentId: '54e9916c99ef4459ae8eabb227a5c341', sequence: '0', text: \"Title:\t\t(Personal Update) Learning AI\n",
    "Author:\t\tJason \n",
    "Posted On:\tThursday, January 18, 2024\n",
    "Topics:\t\tAI, Learning, Azure, Personal Update\n",
    "Summary:\tThis is the first of many blog posts I plan to make this year, stay tuned (please subscribe) for more soon. Learning AI Currently I am working my way through the four stages of competence with the topic of AI. This quarter (Q1 of 2024), I’m currently working on moving from stage 2 to stage 3 in the four stages of competence. For reference, those stages are: Unconscious incompetence Conscious incompetence Conscious competence Unconscious competence Last year I moved from stage 1 to stage 2: In the beginning of last year (2023) I had my head buried in the sand while all the other leaders in my industry were actively learning how to use the latest and greatest AI tool (ChatGPT).\n",
    "\n",
    "Title:\t\tRAG Demo Chronicles Author:\t\tJason\n",
    "Posted On:\tWednesday, February 7, 2024\n",
    "Topics:\t\tAI, Learning, RAG, RAG Demo Series\n",
    "\"})\n",
    "MERGE (learningai:ENTITY { name: 'learningai', type: 'BLOG_POST', id: '1226ef1f38a04c05b13f1f794089cd3e', text: 'Learning AI'})\n",
    "MERGE (learningai)-[:MENTIONED_IN]->(DocumentChunk0)\n",
    "MERGE (jason:ENTITY { name: 'jason', type: 'PERSON', id: '16b00aa6e21e4f30a3ed9577bbf65aba', text: 'Jason'})\n",
    "MERGE (jason)-[:MENTIONED_IN]->(DocumentChunk0)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "List<string> entityCypherText = new List<string>(); // Document, DocumentChunk and Entity\n",
    "\n",
    "entityCypherText.Add($\"MERGE (Document1:DOCUMENT {{ id: '{documentMetatdata.id}', name:'Document1', type:'DOCUMENT', source: '{documentMetatdata.source}'}})\"); \n",
    "\n",
    "foreach (var chunk in chunks.Keys)\n",
    "{\n",
    "    entityCypherText.Add($\"MERGE (DocumentChunk{chunk.sequence}:DOCUMENT_CHUNK {{ id: '{chunk.id}', name: '{chunk.name}', type: 'DOCUMENT_CHUNK', documentId: '{chunk.documentId}', sequence: '{chunk.sequence}', text: \\\"{chunk.text.Replace(\"\\\"\", \"'\")}\\\"}})\");\n",
    "    entityCypherText.Add($\"MERGE (Document1)-[:CONTAINS]->(DocumentChunk{chunk.sequence})\");\n",
    "}\n",
    "\n",
    "HashSet<string> types = new HashSet<string>();\n",
    "foreach(var entity in entities.Keys)\n",
    "{\n",
    "    var labels = entities[entity];\n",
    "    var pcEntity = entity;\n",
    "    entityCypherText.Add($\"MERGE ({pcEntity}:ENTITY {{ name: '{pcEntity}', type: '{labels.type}', id: '{labels.id}', text: '{labels.text}'}})\");\n",
    "\n",
    "    if (!types.Contains(labels.type))\n",
    "    {\n",
    "        types.Add(labels.type);\n",
    "    }\n",
    "\n",
    "    foreach(var key in labels.mentionedInChunks.Keys)\n",
    "    {\n",
    "        var documentChunk = labels.mentionedInChunks[key];\n",
    "        entityCypherText.Add($\"MERGE ({pcEntity})-[:MENTIONED_IN]->(DocumentChunk{documentChunk.sequence})\");\n",
    "    }\n",
    "}\n",
    "\n",
    "HashSet<string> relationships = new HashSet<string>();\n",
    "foreach (ChunkMetadata key in chunks.Keys)\n",
    "{\n",
    "    List<TripletRow> triplets = chunks[key];\n",
    "    foreach (var triplet in triplets)\n",
    "    {\n",
    "        var pcHead = Utilities.CreateName(triplet.head);\n",
    "        var pcTail = Utilities.CreateName(triplet.tail);\n",
    "        entityCypherText.Add($\"MERGE ({pcHead})-[:{triplet.relation.Replace(\" \", \"_\").Replace(\"-\",\"_\")}]->({pcTail})\");\n",
    "\n",
    "        string headRelationship = $\"MERGE (DocumentChunk{key.sequence})-[:MENTIONS]->({pcHead})\";\n",
    "        if (!relationships.Contains(headRelationship))\n",
    "        {\n",
    "            relationships.Add(headRelationship);\n",
    "            entityCypherText.Add(headRelationship);\n",
    "        }\n",
    "        \n",
    "        string tailRelationship = $\"MERGE (DocumentChunk{key.sequence})-[:MENTIONS]->({pcTail})\";\n",
    "        if (!relationships.Contains(tailRelationship))\n",
    "        {\n",
    "            relationships.Add(tailRelationship);\n",
    "            entityCypherText.Add(tailRelationship);\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see all the cypher youc an run this next block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "foreach(var t in entityCypherText)\n",
    "{\n",
    "    Console.WriteLine(t);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see the unique list of entity types you can run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "foreach(var t in types)\n",
    "{\n",
    "    Console.WriteLine(t);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4j connection\n",
    "\n",
    "I've been running this wit Neo4j Desktop. There are other ways to run it. Please check out their [Installation Page](https://neo4j.com/docs/operations-manual/current/installation/) for more information.\n",
    "\n",
    "Once you get a Neo4j database running, you'll need to make sure the information is saved in the .env file mentioned earlier before running this next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "IAuthToken token = AuthTokens.Basic(\n",
    "                envVars[\"NEO4J_USER\"],\n",
    "                envVars[\"NEO4J_PASSWORD\"]\n",
    "            );\n",
    "IDriver driver = GraphDatabase.Driver(envVars[\"NEO4J_URI\"], token);\n",
    "\n",
    "QueryConfig config = new QueryConfig();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some reason you need to debug the cypher text being passed to Neo4j, run this next block to see what the contents are. I had to debug some characters and duplicates getting through the logic when testing. I fixed the bugs I found, but there may be more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "Console.WriteLine(entityCypherText.ToArray().Length);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate Neo4j with the generated cypher text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "using (var session = driver.AsyncSession())\n",
    "{\n",
    "    StringBuilder all = new StringBuilder();\n",
    "    all.AppendJoin(Environment.NewLine, entityCypherText.ToArray());\n",
    "    await driver.ExecutableQuery(all.ToString()).WithConfig(config).ExecuteAsync();\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have enabled two plugins to Neo4j: GenAI, which you'll need for some of the following used features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector index on the DOCUMENT_CHUNK embedding field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "string createVectorIndex = @\"CREATE VECTOR INDEX CHUNK_EMBEDDING IF NOT EXISTS\n",
    "                            FOR (c:DOCUMENT_CHUNK) ON c.embedding\n",
    "                            OPTIONS {indexConfig: {\n",
    "                           `vector.dimensions`: 1536,\n",
    "                            `vector.similarity_function`: 'cosine'\n",
    "                            }}\";\n",
    "\n",
    "await driver.ExecutableQuery(createVectorIndex).WithConfig(config).ExecuteAsync();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a full text index on the entity's text field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "string createFulltextIndex = @\"CREATE FULLTEXT INDEX ENTITY_TEXT IF NOT EXISTS \n",
    "                                FOR (n:ENTITY) ON EACH [n.text]\";\n",
    "await driver.ExecutableQuery(createFulltextIndex).WithConfig(config).ExecuteAsync();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the Vector index using the DOCUMENT_CHUNK text field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "string populateEmbeddings = $@\"\n",
    "                            MATCH (n:DOCUMENT_CHUNK) WHERE n.text IS NOT NULL\n",
    "                            WITH n, genai.vector.encode(\n",
    "                                n.text,\n",
    "                                'AzureOpenAI',\n",
    "                                {{\n",
    "                                    token: $token,\n",
    "                                    resource: $resource,\n",
    "                                    deployment: $deployment\n",
    "                                }}) AS vector\n",
    "                            CALL db.create.setNodeVectorProperty(n, 'embedding', vector)\n",
    "                            \";\n",
    "await driver.ExecutableQuery(populateEmbeddings)\n",
    "    .WithParameters(new() { \n",
    "        {\"token\", envVars[\"AZURE_OPENAI_API_KEY\"]}, \n",
    "        {\"resource\", envVars[\"AZURE_OPENAI_RESOURCE\"]}, \n",
    "        {\"deployment\", envVars[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"]}})\n",
    "    .WithConfig(config)\n",
    "    .ExecuteAsync();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you open the Neo4j Browser for you database and run this command, you should see the entities and relationships:\n",
    "\n",
    "```cypher\n",
    "MATCH (n) RETURN (n)\n",
    "```\n",
    "\n",
    "![Summaries.txt Entities and Relations](.\\images\\summaries-entities-relations.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "Now that we have a graph database populated, we get to decide what sort of retrieval steps we want to include to provide usefal graph data to the RAG workflow.\n",
    "\n",
    "This notebook uses these steps:\n",
    "1. Capture the user's input\n",
    "2. Make a call to the LLM to get 10 keywords or synonyms from the user's request\n",
    "3. Loop through those keyworkds and perform a full text search on the entity's text field and get related entities for the matches found.\n",
    "4. Deduplicate the entities found in step 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "//string questionText = \"what are the blog post titles that are about Semantic Kernel?\";\n",
    "string questionText = \"How many blog post did Jason write about Semantic Kernel and what is their titles?\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonyum extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Semantic Kernel~semantic kernel~related posts~blog entries~Jason~jason~writings~articles~content~titles\r\n"
     ]
    }
   ],
   "source": [
    "ChatClient chatClient = client.GetChatClient(\"chat\");\n",
    "\n",
    "int maxSynonyms = 10;\n",
    "\n",
    "string prompt = $@\"\n",
    "Given a user question, generate related keywords or synonyms up to {maxSynonyms} in total, considering possible cases of capitalization, pluralization, and common expressions. Provide all synonyms/keywords separated by '~' symbols in a single line format: 'synonym1~synonyms2~...'.\n",
    "\n",
    "QUERY: {questionText}\n",
    "######################\n",
    "KEYWORDS:\n",
    "\";\n",
    "ChatCompletion completion = chatClient.CompleteChat(\n",
    "    [\n",
    "        new UserChatMessage(prompt),\n",
    "    ]);\n",
    "\n",
    "Console.WriteLine($\"{completion.Role}: {completion.Content[0].Text}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just one approach to getting additional information from the graph. In this case we do a full text search on the entity text to locate what should be relevant entities in the graph - **then we get their related entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var synonyms = completion.Content[0].Text.Split(\"~\");\n",
    "\n",
    "var uniqueNodes = new HashSet<string>();\n",
    "foreach(var synonym in synonyms)\n",
    "{\n",
    "    Console.WriteLine(synonym);\n",
    "    string cypher = $@\"\n",
    "                        CALL db.index.fulltext.queryNodes(\"\"ENTITY_TEXT\"\", \"\"{synonym}\"\")\n",
    "                        YIELD node AS e1\n",
    "                        MATCH (e1)-[r]-(e2:ENTITY)\n",
    "                        RETURN e1.id, e1.type, e1.text, e2.name, e2.type, e2.text, type(r)\n",
    "                    \";\n",
    "\n",
    "    var textSearchResult = await driver.ExecutableQuery(cypher)\n",
    "                    .WithConfig(config)\n",
    "                    .ExecuteAsync();\n",
    "    if (textSearchResult.Result.Count() > 0)\n",
    "    {\n",
    "        foreach(var r in textSearchResult.Result)\n",
    "        {\n",
    "            var tripletText = $\"{r[\"e1.text\"]} -> {r[\"type(r)\"]} -> {r[\"e2.text\"]}\";\n",
    "            if (!uniqueNodes.Contains(tripletText))\n",
    "            {\n",
    "                uniqueNodes.Add(tripletText);\n",
    "            }   \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"\");\n",
    "Console.WriteLine($\"{uniqueNodes.Count} Unique nodes with matches:\");\n",
    "foreach(var key in uniqueNodes)\n",
    "{\n",
    "    Console.WriteLine($\"{key}\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we perform the typical RAG functionality - a vector similarity search on the document chunk text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "string question = $@\"\n",
    "                    WITH genai.vector.encode(\n",
    "                        $question,\n",
    "                        'AzureOpenAI',\n",
    "                        {{\n",
    "                            token: $token,\n",
    "                            resource: $resource,\n",
    "                            deployment: $deployment\n",
    "                        }}) AS question_embedding\n",
    "                    CALL db.index.vector.queryNodes(\n",
    "                        'CHUNK_EMBEDDING',\n",
    "                        $top_k, \n",
    "                        question_embedding\n",
    "                        ) YIELD node AS chunk, score \n",
    "                    RETURN chunk.id, chunk.text, score\n",
    "                    \";\n",
    "\n",
    "var chunkResult = await driver.ExecutableQuery(question)\n",
    "                .WithParameters(new() { \n",
    "                    {\"question\", questionText},\n",
    "                    {\"token\", envVars[\"AZURE_OPENAI_API_KEY\"]}, \n",
    "                    {\"resource\", envVars[\"AZURE_OPENAI_RESOURCE\"]}, \n",
    "                    {\"deployment\", envVars[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"]},\n",
    "                    {\"top_k\", 5}})\n",
    "                .WithConfig(config)\n",
    "                .ExecuteAsync();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to add the chunk results to the LLM request, I serialize the vector search results as a JSON string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "Console.WriteLine(JsonSerializer.Serialize(chunkResult, new JsonSerializerOptions {\n",
    "             WriteIndented = true\n",
    "         }));\n",
    "\n",
    "StringBuilder chunkTexts = new StringBuilder();\n",
    "foreach(var r in chunkResult.Result)\n",
    "{\n",
    "    chunkTexts.AppendLine($\"Document: {{ text: {r[\"chunk.text\"].ToString()} }}\");\n",
    "}\n",
    "\n",
    "Console.WriteLine(chunkTexts.ToString());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the typical RAG request (no entity or relation information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "ChatClient chatClient = client.GetChatClient(\"chat\");\n",
    "\n",
    "string context = $@\"Unstructured data:\n",
    "{chunkTexts.ToString()}\n",
    "\";\n",
    "\n",
    "string prompt = $@\"Answer the question based only on the following context:\n",
    "\t\t\t    {context}\n",
    "                ######################\n",
    "                Question: {questionText}\n",
    "                ######################\n",
    "                Answer:\";\n",
    "\n",
    "string sysprompt = @\"Be brief in your answers.\n",
    "                    Answer ONLY with the facts listed in the list of sources below. If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below. If asking a clarifying question to the user would help, ask the question.\n",
    "                    For tabular information return it as an html table. Do not return markdown format. If the question is not in English, answer in the language used in the question.\";\n",
    "\n",
    "ChatCompletion completion = chatClient.CompleteChat(\n",
    "    [\n",
    "        new SystemChatMessage(sysprompt),\n",
    "        new UserChatMessage(prompt),\n",
    "    ]);\n",
    "\n",
    "Console.WriteLine($\"{completion.Role}: {completion.Content[0].Text}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the graph RAG request (with entity or relation information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "ChatClient chatClient = client.GetChatClient(\"chat\");\n",
    "\n",
    "string context = $@\"Structured data:\n",
    "    {string.Join(Environment.NewLine, uniqueNodes.ToArray())}\n",
    "Unstructured data:\n",
    "{chunkTexts.ToString()}\n",
    "\";\n",
    "\n",
    "string prompt = $@\"Answer the question based only on the following context:\n",
    "\t\t\t    {context}\n",
    "                ######################\n",
    "                Question: {questionText}\n",
    "                ######################\n",
    "                Answer:\";\n",
    "\n",
    "\n",
    "string sysprompt = @\"Be brief in your answers.\n",
    "                    Answer ONLY with the facts listed in the list of sources below. If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below. If asking a clarifying question to the user would help, ask the question.\n",
    "                    For tabular information return it as an html table. Do not return markdown format. If the question is not in English, answer in the language used in the question.\";\n",
    "\n",
    "ChatCompletion completion = chatClient.CompleteChat(\n",
    "    [\n",
    "        new SystemChatMessage(sysprompt),\n",
    "        new UserChatMessage(prompt),\n",
    "    ]);\n",
    "\n",
    "Console.WriteLine($\"{completion.Role}: {completion.Content[0].Text}\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "python"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
