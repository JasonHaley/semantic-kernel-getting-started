{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Improved) Property Graph RAG in C#\n",
    "\n",
    "Currently this notebook uses the following resources:\n",
    "* Azure Open AI\n",
    "* Neo4j\n",
    "\n",
    "If there is enough interest, I can add the changes needed to just use OpenAI - so if this is something you'd like, let me know on twitter @haleyjason or open an issue on github.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Add the references and using statements used in the rest of the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: Azure.AI.OpenAI, *-*\"\n",
    "#r \"nuget: Azure, *-*\"\n",
    "#r \"nuget: Azure.Identity, *-*\"\n",
    "#r \"nuget: dotenv.net, *-*\"\n",
    "#r \"nuget: Microsoft.DotNet.Interactive.AIUtilities, *-*\"\n",
    "#r \"nuget: Microsoft.ML.Tokenizers, *-*\"\n",
    "#r \"nuget: Microsoft.SemanticKernel.Core, *-*\"\n",
    "#r \"nuget: Neo4j.Driver, *-*\"\n",
    "\n",
    "using Microsoft.DotNet.Interactive;\n",
    "using Microsoft.DotNet.Interactive.AIUtilities;\n",
    "using dotenv.net;\n",
    "using Azure.AI.OpenAI;\n",
    "using Azure;\n",
    "using Azure.Identity;\n",
    "using OpenAI.Chat;\n",
    "using System;\n",
    "using System.Text.Json;\n",
    "using System.Text.Json.Serialization;\n",
    "using System.Text.RegularExpressions;\n",
    "using System.IO;\n",
    "using Microsoft.SemanticKernel.Text;\n",
    "using Microsoft.ML.Tokenizers;\n",
    "using Neo4j.Driver;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the environment variables. **The notebook assumes you have a .env file** with the following contents:\n",
    "\n",
    "```cmd\n",
    "AZURE_OPENAI_ENDPOINT=\"<you azure open ai endpoint>\"\n",
    "AZURE_OPENAI_RESOURCE=\"<you azure open ai resource name>\"\n",
    "AZURE_OPENAI_API_KEY=\"<your azure open ai key>\"\n",
    "AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=\"<name of your embeddings deployment>\"\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT=\"<name of your chat deployment>\"\n",
    "\n",
    "NEO4J_URI=\"neo4j://localhost:7687\"\n",
    "NEO4J_USER=\"<neo4 user name>\"\n",
    "NEO4J_PASSWORD=\"<neo4j user password>\"\n",
    "NEO4J_DATABASE=\"<name of you neo4j database>\",\n",
    "```\n",
    "\n",
    "> Note: I did my testing using text-embedding-ada-002 for embeddings and gpt-4o for the chat service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "DotEnv.Load();\n",
    "\n",
    "var envVars = DotEnv.Read();\n",
    "\n",
    "AzureOpenAIClient client = new(new Uri(envVars[\"AZURE_OPENAI_ENDPOINT\"]), \n",
    "    new AzureKeyCredential(envVars[\"AZURE_OPENAI_API_KEY\"]));\n",
    "\n",
    "var embeddings = envVars[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"];\n",
    "var llm = envVars[\"AZURE_OPENAI_CHAT_DEPLOYMENT\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4j connection\n",
    "\n",
    "I've been running this wit Neo4j Desktop. There are other ways to run it. Please check out their [Installation Page](https://neo4j.com/docs/operations-manual/current/installation/) for more information.\n",
    "\n",
    "Once you get a Neo4j database running, you'll need to make sure the information is saved in the .env file mentioned earlier before running this next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "IAuthToken token = AuthTokens.Basic(\n",
    "                envVars[\"NEO4J_USER\"],\n",
    "                envVars[\"NEO4J_PASSWORD\"]\n",
    "            );\n",
    "IDriver driver = GraphDatabase.Driver(envVars[\"NEO4J_URI\"], token);\n",
    "\n",
    "QueryConfig config = new QueryConfig();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ingestion phase is broken up in to the following steps, which should allow for some experimentation with the different steps:\n",
    "1. define the data structures used in extracting the entities and populating the Neo4j database\n",
    "2. call the LLM to extract entities\n",
    "3. process the results into a unique list of entities and their relationships\n",
    "4. generate the cypher to populate Neo4j\n",
    "5. populate the Neo4j database\n",
    "6. create and populate vector and full text indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the data structures and utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "public record DocunentMetadata(string id, string source);\n",
    "public record ChunkMetadata(string id, string name, int sequence, string documentId, string text);\n",
    "public record TripletRow(string head, string head_type, string relation, string tail, string tail_type);\n",
    "public class EntityMetadata\n",
    "{\n",
    "    public string name { get; set; }\n",
    "    public string type { get; set; }\n",
    "    public string id { get; set; }\n",
    "    public string text { get; set; }\n",
    "    public Dictionary<string, ChunkMetadata> mentionedInChunks {get; set;} = new Dictionary<string, ChunkMetadata>();\n",
    "}\n",
    "\n",
    "public class Utilities\n",
    "{    \n",
    "    public static EntityMetadata PopulateEntityMetadata(ChunkMetadata chunkMetadata, TripletRow triplet, EntityMetadata entityMetadata, bool isHead = true)\n",
    "    {\n",
    "        entityMetadata.id = Guid.NewGuid().ToString(\"N\");\n",
    "\n",
    "        if (isHead)\n",
    "        {\n",
    "            entityMetadata.name = CreateName(triplet.head);\n",
    "            entityMetadata.type = triplet.head_type;\n",
    "            entityMetadata.text = triplet.head;\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            entityMetadata.name = CreateName(triplet.tail);\n",
    "            entityMetadata.type = triplet.tail_type;\n",
    "            entityMetadata.text = triplet.tail;\n",
    "        }\n",
    "\n",
    "        entityMetadata.mentionedInChunks.Add(chunkMetadata.id, chunkMetadata);\n",
    "        \n",
    "        return entityMetadata;\n",
    "    }\n",
    "\n",
    "    public static string CreateName(string text)\n",
    "    {\n",
    "        if (string.IsNullOrEmpty(text))\n",
    "            return text;\n",
    "\n",
    "        // Split the text into words\n",
    "        string[] words = text.Split(new[] { ' ', '-', '_' }, StringSplitOptions.RemoveEmptyEntries);\n",
    "\n",
    "        StringBuilder nameText = new StringBuilder();\n",
    "        \n",
    "        foreach (string word in words)\n",
    "        {\n",
    "            // Capitalize the first letter and make the rest lowercase\n",
    "            var lword = word;\n",
    "            if (char.IsDigit(word[0]))\n",
    "            {\n",
    "                lword = \"_\" + word;\n",
    "            }\n",
    "\n",
    "            nameText.Append(lword.ToLower());\n",
    "        }\n",
    "        return Regex.Replace(nameText.ToString(), \"[^a-zA-Z0-9_]\", \"\");\n",
    "    }\n",
    "    \n",
    "    public static List<string> SplitPlainTextOnEmptyLine(string[] lines)\n",
    "    {\n",
    "        List<string> allLines = new List<string>(lines);\n",
    "        List<string> result = new List<string>();\n",
    "\n",
    "        // Make sure there is an empty string as last line to split into paragraph\n",
    "        var last = allLines.Last();\n",
    "        if (last.Length > 0)\n",
    "        {\n",
    "            allLines.Add(\"\");\n",
    "        }\n",
    "\n",
    "        StringBuilder paragraphBuilder = new StringBuilder();\n",
    "        foreach (string input in allLines)\n",
    "        {\n",
    "            if (input.Length == 0)\n",
    "            {\n",
    "                result.Add(paragraphBuilder.ToString());\n",
    "                paragraphBuilder.Clear();\n",
    "            }\n",
    "            paragraphBuilder.Append($\"{input} \");\n",
    "        }\n",
    "\n",
    "        return result;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity extraction\n",
    "\n",
    "This step is where the LLM takes the chunks of text and extracts up to 10 entities per chunk\n",
    "\n",
    "Steps include:\n",
    "* Chunk the data into individual summaries (this was changed from the initial notebook)\n",
    "* Provide some default entities and relation types for the prompt to use in directing the LLM in extracting the entities (extration works best if you customize this to match your data file contents)\n",
    "* Loop through all the chunks calling the LLM for each chunk **(Warning: this can get expensive - so change the ```paragraphs.Count``` limit to 1 or 2 until you are happy with your results)**\n",
    "* Parse each JSON result form the LLM calls and keep the ```chunks``` variable for later post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "ChatClient chatClient = client.GetChatClient(llm);\n",
    "string fileName = \"data/summaries.txt\";\n",
    "string fileText = File.ReadAllText(fileName);\n",
    "\n",
    "DocunentMetadata documentMetatdata = new (Guid.NewGuid().ToString(\"N\"), fileName);\n",
    "\n",
    "var simpleLines = File.ReadAllLines(documentMetatdata.source);\n",
    "var paragraphs = Utilities.SplitPlainTextOnEmptyLine(simpleLines);\n",
    "\n",
    "string entityTypes = \"BLOG_POST,PRESENTATION,EVENT,ORGANIZATION,PERSON,PLACE,TECHNOLOGY,SOFTWARE_SYSTEM,REVIEW,ACTION\";\n",
    "string relationTypes = \"WRITTEN_BY,PRESENTED_BY,PART_OF,LOCATED_IN,LIVES_IN,TRAVELED_TO\";\n",
    "\n",
    "Dictionary<ChunkMetadata, List<TripletRow>> chunks = new Dictionary<ChunkMetadata, List<TripletRow>>();\n",
    "int maxTripletsPerChunk = 20;\n",
    "string preamble = \"The given text document contains blog entry summaries with a Title, Author, Posted On date, Topics and Summary. Make sure to add the WRITTEN_BY relationship for the author.\";\n",
    "for (int i = 0; i < paragraphs.Count; i++)\n",
    "{\n",
    "    string text = paragraphs[i];\n",
    "\n",
    "    ChunkMetadata chunkMetadata = new (Guid.NewGuid().ToString(\"N\"), $\"DocumentChunk{i}\", i, documentMetatdata.id, text);\n",
    "\n",
    "\tstring prompt =  $@\"Please extract up to {maxTripletsPerChunk} knowledge triplets from the provied text.\n",
    "    {{$preamble}}\n",
    "    Each triplet should be in the form of (head, relation, tail) with their respective types.\n",
    "    ######################\n",
    "    ONTOLOGY:\n",
    "    Entity Types: {entityTypes}\n",
    "    Relation Types: {relationTypes}\n",
    "    \n",
    "    Use these entity types and relation types as a starting point, introduce new types if necessary based on the context.\n",
    "    \n",
    "    GUIDELINES:\n",
    "    - Output in JSON format: [{{\"\"head\"\": \"\"\"\", \"\"head_type\"\": \"\"\"\", \"\"relation\"\": \"\"\"\", \"\"tail\"\": \"\"\"\", \"\"tail_type\"\": \"\"\"\"}}]\n",
    "    - Use the full form for entities (ie., 'Artificial Intelligence' instead of 'AI')\n",
    "    - Keep entities and relation names concise (3-5 words max)\n",
    "    - Break down complex phrases into multiple triplets\n",
    "    - Ensure the knowledge graph is coherent and easily understandable\n",
    "    ######################\n",
    "    EXAMPLE:\n",
    "    Text: Jason Haley, chief engineer of Jason Haley Consulting, wrote a new blog post titled 'Study Notes: GraphRAG - Property Graphs' about creating a property graph RAG system using Semantic Kernel. \n",
    "    Output:\n",
    "    [{{\"\"head\"\": \"\"Jason Haley\"\", \"\"head_type\"\": \"\"PERSON\"\", \"\"relation\"\": \"\"WORKS_FOR\"\", \"\"tail\"\": \"\"Jason Haley Consulting\"\", \"\"tail_type\"\": \"\"COMPANY\"\"}},\n",
    "    {{\"\"head\"\": \"\"Study Notes: GraphRAG - Property Grids\"\", \"\"head_type\"\": \"\"BLOG_POST\"\", \"\"relation\"\": \"\"WRITTEN_BY\"\", \"\"tail\"\": \"\"Jason Haley\"\", \"\"tail_type\"\": \"\"PERSON\"\"}},\n",
    "    {{\"\"head\"\": \"\"Study Notes: GraphRAG - Property Grids\"\", \"\"head_type\"\": \"\"BLOG_POST\"\", \"\"relation\"\": \"\"TOPIC\"\", \"\"tail\"\": \"\"Semantic Kernel\"\", \"\"tail_type\"\": \"\"TECHNOLOGY\"\"}},\n",
    "    {{\"\"head\"\": \"\"property grid RAG system\"\", \"\"head_type\"\": \"\"SOFTWARE_SYSTEM\"\", \"\"relation\"\": \"\"USES\"\", \"\"tail\"\": \"\"Semantic Kernel\"\", \"\"tail_type\"\": \"\"TECHNOLOGY\"\"}}]\n",
    "    ######################\n",
    "    Text: {text}\n",
    "    ######################\n",
    "    Output:\";\n",
    "\n",
    "\tChatCompletion completion = chatClient.CompleteChat(\n",
    "    \t[\n",
    "        \tnew UserChatMessage(prompt),\n",
    "    \t]);\n",
    "\n",
    "\tConsole.WriteLine($\"{completion.Role}: {completion.Content[0].Text}\");\n",
    "    List<TripletRow> rows =  JsonSerializer.Deserialize<List<TripletRow>>(completion.Content[0].Text.Replace(\"```json\", \"\").Replace(\"```\",\"\").Replace(\"'\", \"\").Trim());\n",
    "    \n",
    "    chunks.Add(chunkMetadata, rows);\n",
    "}\n",
    "\n",
    "Console.WriteLine($\"Number of chunks: {chunks.Count}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the LLM results and create a dictionary of the entitites. In order to create a relation from each entity to the document chunk it was extracted from we also keep a mentionedInChunk dictionary (this could be a 1 to many relationship)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "Dictionary<string,EntityMetadata> entities = new Dictionary<string,EntityMetadata>();\n",
    "\n",
    "foreach (ChunkMetadata key in chunks.Keys)\n",
    "{\n",
    "    List<TripletRow> triplets = chunks[key];\n",
    "    foreach (var triplet in triplets)\n",
    "    {\n",
    "        EntityMetadata entity;\n",
    "        string pcHead = Utilities.CreateName(triplet.head);\n",
    "        if (entities.ContainsKey(pcHead)) \n",
    "        {\n",
    "            entity = entities[pcHead];\n",
    "            if (!entity.mentionedInChunks.ContainsKey(key.id))\n",
    "            {\n",
    "                entity.mentionedInChunks.Add(key.id, key);\n",
    "            }\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            entity = new EntityMetadata();   \n",
    "            entities.Add(pcHead, Utilities.PopulateEntityMetadata(key, triplet, entity, true));\n",
    "        }      \n",
    "\n",
    "        string pcTail = Utilities.CreateName(triplet.tail);\n",
    "        if (entities.ContainsKey(pcTail)) \n",
    "        {\n",
    "            entity = entities[pcTail];\n",
    "            if (!entity.mentionedInChunks.ContainsKey(key.id))\n",
    "            {\n",
    "                entity.mentionedInChunks.Add(key.id, key);\n",
    "            }\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            entity = new EntityMetadata();   \n",
    "            entities.Add(pcTail, Utilities.PopulateEntityMetadata(key, triplet, entity, false));\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "Console.WriteLine($\"Unique entity count: {entities.Count}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see the entities and list of which document chunks they were extracted from, you can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "foreach(var key in entities.Keys)\n",
    "{\n",
    "    var e = entities[key];\n",
    "    Console.WriteLine($\"{key} Mentioned In {e.mentionedInChunks.Count} chunks\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is all about generating the cypher to populate the entities extracted by the LLM into Neo4j.\n",
    "\n",
    "The results of this step look something like this:\n",
    "```cypher\n",
    "MERGE (Document1:DOCUMENT { id: '54e9916c99ef4459ae8eabb227a5c341', name:'Document1', type:'DOCUMENT', source: 'data/summaries.txt'})\n",
    "MERGE (DocumentChunk0:DOCUMENT_CHUNK { id: '8dcf15992ced4c8ba77e9dd6f9372241', name: 'DocumentChunk0', type: 'DOCUMENT_CHUNK', documentId: '54e9916c99ef4459ae8eabb227a5c341', sequence: '0', text: \"Title:\t\t(Personal Update) Learning AI\n",
    "Author:\t\tJason \n",
    "Posted On:\tThursday, January 18, 2024\n",
    "Topics:\t\tAI, Learning, Azure, Personal Update\n",
    "Summary:\tThis is the first of many blog posts I plan to make this year, stay tuned (please subscribe) for more soon. Learning AI Currently I am working my way through the four stages of competence with the topic of AI. This quarter (Q1 of 2024), I’m currently working on moving from stage 2 to stage 3 in the four stages of competence. For reference, those stages are: Unconscious incompetence Conscious incompetence Conscious competence Unconscious competence Last year I moved from stage 1 to stage 2: In the beginning of last year (2023) I had my head buried in the sand while all the other leaders in my industry were actively learning how to use the latest and greatest AI tool (ChatGPT).\n",
    "\n",
    "Title:\t\tRAG Demo Chronicles Author:\t\tJason\n",
    "Posted On:\tWednesday, February 7, 2024\n",
    "Topics:\t\tAI, Learning, RAG, RAG Demo Series\n",
    "\"})\n",
    "MERGE (learningai:ENTITY { name: 'learningai', type: 'BLOG_POST', id: '1226ef1f38a04c05b13f1f794089cd3e', text: 'Learning AI'})\n",
    "MERGE (learningai)-[:MENTIONED_IN]->(DocumentChunk0)\n",
    "MERGE (jason:ENTITY { name: 'jason', type: 'PERSON', id: '16b00aa6e21e4f30a3ed9577bbf65aba', text: 'Jason'})\n",
    "MERGE (jason)-[:MENTIONED_IN]->(DocumentChunk0)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "List<string> entityCypherText = new List<string>(); // Document, DocumentChunk and Entity\n",
    "\n",
    "entityCypherText.Add($\"MERGE (Document1:DOCUMENT {{ id: '{documentMetatdata.id}', name:'Document1', type:'DOCUMENT', source: '{documentMetatdata.source}'}})\"); \n",
    "\n",
    "foreach (var chunk in chunks.Keys)\n",
    "{\n",
    "    entityCypherText.Add($\"MERGE (DocumentChunk{chunk.sequence}:DOCUMENT_CHUNK {{ id: '{chunk.id}', name: '{chunk.name}', type: 'DOCUMENT_CHUNK', documentId: '{chunk.documentId}', sequence: '{chunk.sequence}', text: \\\"{chunk.text.Replace(\"\\\"\", \"'\")}\\\"}})\");\n",
    "    entityCypherText.Add($\"MERGE (Document1)-[:CONTAINS]->(DocumentChunk{chunk.sequence})\");\n",
    "}\n",
    "\n",
    "HashSet<string> types = new HashSet<string>();\n",
    "foreach(var entity in entities.Keys)\n",
    "{\n",
    "    var labels = entities[entity];\n",
    "    var pcEntity = entity;\n",
    "\n",
    "    // Handle strange issue when type is empty string\n",
    "    if (string.IsNullOrEmpty(labels.type))\n",
    "    {\n",
    "        continue;\n",
    "    }\n",
    "    entityCypherText.Add($\"MERGE ({pcEntity}:ENTITY {{ name: '{pcEntity}', type: '{labels.type}', id: '{labels.id}', text: '{labels.text}'}})\");\n",
    "\n",
    "    if (!types.Contains(labels.type))\n",
    "    {\n",
    "        types.Add(labels.type);\n",
    "    }\n",
    "\n",
    "    foreach(var key in labels.mentionedInChunks.Keys)\n",
    "    {\n",
    "        var documentChunk = labels.mentionedInChunks[key];\n",
    "        entityCypherText.Add($\"MERGE ({pcEntity})-[:MENTIONED_IN]->(DocumentChunk{documentChunk.sequence})\");\n",
    "    }\n",
    "}\n",
    "\n",
    "HashSet<string> relationships = new HashSet<string>();\n",
    "foreach (ChunkMetadata key in chunks.Keys)\n",
    "{\n",
    "    List<TripletRow> triplets = chunks[key];\n",
    "    foreach (var triplet in triplets)\n",
    "    {\n",
    "        var pcHead = Utilities.CreateName(triplet.head);\n",
    "        var pcTail = Utilities.CreateName(triplet.tail);\n",
    "        var relationName = triplet.relation.Replace(\" \", \"_\").Replace(\"-\",\"_\");\n",
    "        if (string.IsNullOrEmpty(relationName))\n",
    "        {\n",
    "            relationName = \"RELATED_TO\";\n",
    "        }\n",
    "        entityCypherText.Add($\"MERGE ({pcHead})-[:{relationName}]->({pcTail})\");\n",
    "\n",
    "        string headRelationship = $\"MERGE (DocumentChunk{key.sequence})-[:MENTIONS]->({pcHead})\";\n",
    "        if (!relationships.Contains(headRelationship))\n",
    "        {\n",
    "            relationships.Add(headRelationship);\n",
    "            entityCypherText.Add(headRelationship);\n",
    "        }\n",
    "        \n",
    "        string tailRelationship = $\"MERGE (DocumentChunk{key.sequence})-[:MENTIONS]->({pcTail})\";\n",
    "        if (!relationships.Contains(tailRelationship))\n",
    "        {\n",
    "            relationships.Add(tailRelationship);\n",
    "            entityCypherText.Add(tailRelationship);\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see all the cypher youc an run this next block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "foreach(var t in entityCypherText)\n",
    "{\n",
    "    Console.WriteLine(t);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see the unique list of entity types you can run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "foreach(var t in types)\n",
    "{\n",
    "    Console.WriteLine(t);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the graph db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some reason you need to debug the cypher text being passed to Neo4j, run this next block to see what the contents are. I had to debug some characters and duplicates getting through the logic when testing. I fixed the bugs I found, but there may be more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "Console.WriteLine(entityCypherText.ToArray().Length);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate Neo4j with the generated cypher text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "StringBuilder all = new StringBuilder();\n",
    "all.AppendJoin(Environment.NewLine, entityCypherText.ToArray());\n",
    "await driver.ExecutableQuery(all.ToString()).WithConfig(config).ExecuteAsync();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have enabled two plugins to Neo4j: GenAI, which you'll need for some of the following used features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector index on the DOCUMENT_CHUNK embedding field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "string createVectorIndex = @\"CREATE VECTOR INDEX CHUNK_EMBEDDING IF NOT EXISTS\n",
    "                            FOR (c:DOCUMENT_CHUNK) ON c.embedding\n",
    "                            OPTIONS {indexConfig: {\n",
    "                           `vector.dimensions`: 1536,\n",
    "                            `vector.similarity_function`: 'cosine'\n",
    "                            }}\";\n",
    "\n",
    "await driver.ExecutableQuery(createVectorIndex).WithConfig(config).ExecuteAsync();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the Vector index using the DOCUMENT_CHUNK text field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "string populateEmbeddings = $@\"\n",
    "                            MATCH (n:DOCUMENT_CHUNK) WHERE n.text IS NOT NULL\n",
    "                            WITH n, genai.vector.encode(\n",
    "                                n.text,\n",
    "                                'AzureOpenAI',\n",
    "                                {{\n",
    "                                    token: $token,\n",
    "                                    resource: $resource,\n",
    "                                    deployment: $deployment\n",
    "                                }}) AS vector\n",
    "                            CALL db.create.setNodeVectorProperty(n, 'embedding', vector)\n",
    "                            \";\n",
    "await driver.ExecutableQuery(populateEmbeddings)\n",
    "    .WithParameters(new() { \n",
    "        {\"token\", envVars[\"AZURE_OPENAI_API_KEY\"]}, \n",
    "        {\"resource\", envVars[\"AZURE_OPENAI_RESOURCE\"]}, \n",
    "        {\"deployment\", envVars[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"]}})\n",
    "    .WithConfig(config)\n",
    "    .ExecuteAsync();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a vector index to the ENTITY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "string createEntityVectorIndex = @\"CREATE VECTOR INDEX TEXT_EMBEDDING IF NOT EXISTS\n",
    "                                    FOR (e:ENTITY) ON e.embedding\n",
    "                                    OPTIONS {indexConfig: {\n",
    "                                        `vector.dimensions`: 1536,\n",
    "                                        `vector.similarity_function`: 'cosine'\n",
    "                                    }}\";\n",
    "\n",
    "await driver.ExecutableQuery(createEntityVectorIndex).WithConfig(config).ExecuteAsync();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the ENTITITY vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "string populateEntittyEmbeddings = $@\"\n",
    "                            MATCH (n:ENTITY) WHERE n.text IS NOT NULL\n",
    "                            WITH n, genai.vector.encode(\n",
    "                                n.text,\n",
    "                                'AzureOpenAI',\n",
    "                                {{\n",
    "                                    token: $token,\n",
    "                                    resource: $resource,\n",
    "                                    deployment: $deployment\n",
    "                                }}) AS vector\n",
    "                            CALL db.create.setNodeVectorProperty(n, 'embedding', vector)\n",
    "                            \";\n",
    "await driver.ExecutableQuery(populateEntittyEmbeddings)\n",
    "    .WithParameters(new() { \n",
    "        {\"token\", envVars[\"AZURE_OPENAI_API_KEY\"]}, \n",
    "        {\"resource\", envVars[\"AZURE_OPENAI_RESOURCE\"]}, \n",
    "        {\"deployment\", envVars[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"]}})\n",
    "    .WithConfig(config)\n",
    "    .ExecuteAsync();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a full text index on the entity's text field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "string createFulltextIndex = @\"CREATE FULLTEXT INDEX ENTITY_TEXT IF NOT EXISTS \n",
    "                                FOR (n:ENTITY) ON EACH [n.text]\";\n",
    "await driver.ExecutableQuery(createFulltextIndex).WithConfig(config).ExecuteAsync();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you open the Neo4j Browser for you database and run this command, you should see the entities and relationships:\n",
    "\n",
    "```cypher\n",
    "MATCH (n) RETURN (n)\n",
    "```\n",
    "\n",
    "![Summaries.txt Entities and Relations](.\\images\\summaries-entities-relations.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "Now that we have a graph database populated, we get to decide what sort of retrieval steps we want to include to provide usefal graph data to the RAG workflow.\n",
    "\n",
    "This notebook uses these steps:\n",
    "1. Capture the user's input\n",
    "2. Make a call to the LLM to get a keyword that sums up the user's request (this is a change from the first notebook)\n",
    "3. (Optionally) do a full text search on entities\n",
    "4. Do a vector search on the entity text for the keyword extracted in #2\n",
    "5. Deduplicate the entities found in step 4\n",
    "6. Do vector similarity search on the document chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "//string questionText = \"what are the blog post titles that are about Semantic Kernel?\";\n",
    "string questionText = \"How many blog post did Jason write about Semantic Kernel and what are their titles?\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "ChatClient chatClient = client.GetChatClient(\"chat\");\n",
    "\n",
    "int maxSynonyms = 10;\n",
    "string prompt = $@\"\n",
    "Given a user question, pick or use 1 to 3 words to create a keyword to capture what the user is asking for'.\n",
    "\n",
    "QUERY: {questionText}\n",
    "######################\n",
    "KEYWORDS:\n",
    "\";\n",
    "ChatCompletion completion = chatClient.CompleteChat(\n",
    "    [\n",
    "        new UserChatMessage(prompt),\n",
    "    ]);\n",
    "\n",
    "Console.WriteLine($\"{completion.Role}: {completion.Content[0].Text}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure for search results and scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "public record FulltextResult(string text, double score);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just one approach to getting additional information from the graph. \n",
    "\n",
    "If you want do do a full text search on the entity text and get the related entities, that could also be done with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var synonyms = completion.Content[0].Text.Split(\"~\");\n",
    "\n",
    "var uniqueNodes = new HashSet<FulltextResult>();\n",
    "foreach(var synonym in synonyms)\n",
    "{\n",
    "    Console.WriteLine(synonym);\n",
    "    string cypher = $@\"\n",
    "                        CALL db.index.fulltext.queryNodes(\"\"ENTITY_TEXT\"\", \"\"{synonym}\"\")\n",
    "                        YIELD node AS e1, score\n",
    "                        MATCH (e1)-[r]-(e2:ENTITY)\n",
    "                        RETURN '(' + COALESCE(e1.text,'') + ')-[:' + COALESCE(type(r),'') + ']->(' + COALESCE(e2.text,'') + ')' as triplet, score\n",
    "                    \";\n",
    "\n",
    "    var textSearchResult = await driver.ExecutableQuery(cypher)\n",
    "                    .WithConfig(config)\n",
    "                    .ExecuteAsync();\n",
    "    if (textSearchResult.Result.Count() > 0)\n",
    "    {\n",
    "        foreach(var r in textSearchResult.Result)\n",
    "        {\n",
    "            var tripletText = $\"{r[\"triplet\"]}\";\n",
    "            var fullTextResult = new FulltextResult(tripletText, Convert.ToDouble(r[\"score\"]));\n",
    "            if (!uniqueNodes.Contains(fullTextResult))\n",
    "        {\n",
    "            uniqueNodes.Add(fullTextResult);\n",
    "            Console.WriteLine($\"{fullTextResult.text} {fullTextResult.score}\");\n",
    "        }  \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"\");\n",
    "Console.WriteLine($\"{uniqueNodes.Count} Unique nodes with matches:\");\n",
    "foreach(var key in uniqueNodes)\n",
    "{\n",
    "    Console.WriteLine($\"{key}\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a vector search for the keyword on the entity text field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "string question = $@\"\n",
    "                    WITH genai.vector.encode(\n",
    "                            $question,\n",
    "                            'AzureOpenAI',\n",
    "                            {{\n",
    "                                token: $token,\n",
    "                                resource: $resource,\n",
    "                                deployment: $deployment\n",
    "                            }}) AS question_embedding\n",
    "                        CALL db.index.vector.queryNodes(\n",
    "                            'TEXT_EMBEDDING',\n",
    "                            $top_k, \n",
    "                            question_embedding\n",
    "                            ) \n",
    "                        YIELD node AS e1, score\n",
    "                        MATCH (e1)-[r]-(e2:ENTITY)-[r2:MENTIONED_IN]->(dc)\n",
    "                        RETURN '(' + COALESCE(e1.text,'') + ')-[:' + COALESCE(type(r),'') + ']->(' + COALESCE(e2.text,'') + ')' as triplet, dc.text as t, score\n",
    "                    \";\n",
    "\n",
    "var chunkResult = await driver.ExecutableQuery(question)\n",
    "                .WithParameters(new() { \n",
    "                    {\"question\", questionText},\n",
    "                    {\"token\", envVars[\"AZURE_OPENAI_API_KEY\"]}, \n",
    "                    {\"resource\", envVars[\"AZURE_OPENAI_RESOURCE\"]}, \n",
    "                    {\"deployment\", envVars[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"]},\n",
    "                    {\"top_k\", 5}})\n",
    "                .WithConfig(config)\n",
    "                .ExecuteAsync();\n",
    "\n",
    "var uniqueNodes = new HashSet<FulltextResult>();\n",
    "if (chunkResult.Result.Count() > 0)\n",
    "{\n",
    "    foreach(var r in chunkResult.Result)\n",
    "    {\n",
    "        var tripletText = $\"{r[\"triplet\"]}\";\n",
    "        var fullTextResult = new FulltextResult(tripletText, Convert.ToDouble(r[\"score\"]));\n",
    "        if (!uniqueNodes.Contains(fullTextResult))\n",
    "        {\n",
    "            uniqueNodes.Add(fullTextResult);\n",
    "            Console.WriteLine($\"{fullTextResult.text} {fullTextResult.score}\");\n",
    "        }   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we perform the typical RAG functionality - a vector similarity search on the document chunk text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "string question = $@\"\n",
    "                    WITH genai.vector.encode(\n",
    "                        $question,\n",
    "                        'AzureOpenAI',\n",
    "                        {{\n",
    "                            token: $token,\n",
    "                            resource: $resource,\n",
    "                            deployment: $deployment\n",
    "                        }}) AS question_embedding\n",
    "                    CALL db.index.vector.queryNodes(\n",
    "                        'CHUNK_EMBEDDING',\n",
    "                        $top_k, \n",
    "                        question_embedding\n",
    "                        ) YIELD node AS chunk, score \n",
    "                    RETURN chunk.id, chunk.text, score\n",
    "                    \";\n",
    "\n",
    "var chunkResult = await driver.ExecutableQuery(question)\n",
    "                .WithParameters(new() { \n",
    "                    {\"question\", questionText},\n",
    "                    {\"token\", envVars[\"AZURE_OPENAI_API_KEY\"]}, \n",
    "                    {\"resource\", envVars[\"AZURE_OPENAI_RESOURCE\"]}, \n",
    "                    {\"deployment\", envVars[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"]},\n",
    "                    {\"top_k\", 5}})\n",
    "                .WithConfig(config)\n",
    "                .ExecuteAsync();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to add the chunk results to the LLM request, I serialize the vector search results as a JSON string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "Console.WriteLine(JsonSerializer.Serialize(chunkResult, new JsonSerializerOptions {\n",
    "             WriteIndented = true\n",
    "         }));\n",
    "\n",
    "StringBuilder chunkTexts = new StringBuilder();\n",
    "foreach(var r in chunkResult.Result)\n",
    "{\n",
    "    chunkTexts.AppendLine($\"Document: {{ text: {r[\"chunk.text\"].ToString()} }}\");\n",
    "}\n",
    "\n",
    "Console.WriteLine(chunkTexts.ToString());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the typical RAG request (no entity or relation information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "ChatClient chatClient = client.GetChatClient(\"chat\");\n",
    "\n",
    "string context = $@\"Unstructured data:\n",
    "{chunkTexts.ToString()}\n",
    "\";\n",
    "\n",
    "string prompt = $@\"Answer the question based only on the following context:\n",
    "\t\t\t    {context}\n",
    "                ######################\n",
    "                Question: {questionText}\n",
    "                ######################\n",
    "                Answer:\";\n",
    "\n",
    "string sysprompt = @\"Be brief in your answers.\n",
    "                    Answer ONLY with the facts listed in the list of sources below. If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below. If asking a clarifying question to the user would help, ask the question.\n",
    "                    For tabular information return it as an html table. Do not return markdown format. If the question is not in English, answer in the language used in the question.\";\n",
    "\n",
    "ChatCompletion completion = chatClient.CompleteChat(\n",
    "    [\n",
    "        new SystemChatMessage(sysprompt),\n",
    "        new UserChatMessage(prompt),\n",
    "    ]);\n",
    "\n",
    "Console.WriteLine($\"{completion.Role}: {completion.Content[0].Text}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the graph RAG request (with entity or relation information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "ChatClient chatClient = client.GetChatClient(\"chat\");\n",
    "\n",
    "string context = $@\"\n",
    "######################\n",
    "Structured data:\n",
    "{string.Join(Environment.NewLine, uniqueNodes.Select(c => c.text).Take(50).ToArray())}\n",
    "######################\n",
    "Unstructured data:\n",
    "{chunkTexts.ToString()}\n",
    "\";\n",
    "\n",
    "string prompt = $@\"\n",
    "To plan the response, begin by examining the Neo4j entity relations and their structured data to determine if the answer is present within. Follow these steps:\n",
    "\n",
    "Analyze the provided Neo4j entity relations and their structured data:\n",
    "\n",
    "Look at the nodes, relationships, and properties in the graph.\n",
    "Identify the entities and their connections relevant to the question.\n",
    "Identify relevant information:\n",
    "\n",
    "Extract data points and relationships that are pertinent to the question.\n",
    "Consider how these relationships influence the answer.\n",
    "Synthesize the identified information:\n",
    "\n",
    "Combine the extracted information logically.\n",
    "Formulate a coherent and comprehensive response.\n",
    "Here are some examples to guide the process:\n",
    "\n",
    "######################\n",
    "Example:\n",
    "(Semantic Kernel)-[:TOPIC]->(Blog Post Title 1)\n",
    "(Semantic Kernel)-[:HAS_TOPIC]->(Blog Post Title 2)\n",
    "(Semantic Kernel)-[:INCLUDES_TOPIC]->(Blog Post Title 3)\n",
    "\n",
    "Question:\n",
    "What blog posts are about Semantic Kernel?\n",
    "\n",
    "Answer:\n",
    "Blog Post is about Semantic Kernel\n",
    "######################\n",
    "Answer the question based solely on the following context:\n",
    "{context}\n",
    "\n",
    "######################\n",
    "Question: {questionText}\n",
    "######################\n",
    "Answer:\";\n",
    "\n",
    "string sysprompt = @\"Answer ONLY with the facts listed in the list of sources below. If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below. If asking a clarifying question to the user would help, ask the question.\n",
    "                    For tabular information return it as an html table. Do not return markdown format. If the question is not in English, answer in the language used in the question.\";\n",
    "\n",
    "ChatCompletion completion = chatClient.CompleteChat(\n",
    "    [\n",
    "        new SystemChatMessage(sysprompt),\n",
    "        new UserChatMessage(prompt),\n",
    "    ]);\n",
    "\n",
    "Console.WriteLine($\"{completion.Role}: {completion.Content[0].Text}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what the prompt was to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Console.WriteLine(sysprompt);\n",
    "Console.WriteLine(\"######################\");\n",
    "Console.WriteLine(prompt);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "python"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
